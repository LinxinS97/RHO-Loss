{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial notebook\n",
    "\n",
    "This notebook runs the full training pipeline, including:\n",
    "* training the irreducible loss model on the holdout set\n",
    "* training the target model\n",
    "\n",
    "The dataset is CIFAR-10, both target model and irreducible loss model have a Resnet-18 architecture.\n",
    "\n",
    "Note: Before you can run this, you need to install the dependencies and activate the environment (see readme)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /auto/users/janner/goldiprox-hydra/src/datamodules\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from pytorch_lightning import (\n",
    "    Callback,\n",
    "    LightningDataModule,\n",
    "    LightningModule,\n",
    "    Trainer,\n",
    "    seed_everything,\n",
    ")\n",
    "from pytorch_lightning.loggers import LightningLoggerBase\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.utils import utils\n",
    "from src.models.OneModel import OneModel\n",
    "\n",
    "log = utils.get_logger(__name__)\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Irreducible loss model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create config\n",
    "\n",
    "We manage config files with Hydra. For this tutorial, we just create the whole config here as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to download the datasets\n",
    "data_dir = \"/path/to/dir/\"\n",
    "\n",
    "# where to upload the weights and biases logs\n",
    "my_project = \"tutorial_notebook\"\n",
    "my_entity = \"xyz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"trainer\": {\n",
    "        \"_target_\": \"pytorch_lightning.Trainer\",\n",
    "        \"gpus\": 1,\n",
    "        \"min_epochs\": 1,\n",
    "        \"max_epochs\": 100,\n",
    "        \"weights_summary\": None,\n",
    "        \"progress_bar_refresh_rate\": 20,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"_target_\": \"src.models.OneModel.OneModel\",\n",
    "        \"model\": {\n",
    "            \"_target_\": \"src.models.modules.resnet_cifar.ResNet18\"\n",
    "        },\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"_target_\": \"torch.optim.AdamW\",\n",
    "        \"lr\": 0.001\n",
    "    },\n",
    "    \"datamodule\": {\n",
    "        \"_target_\": \"src.datamodules.datamodules.CIFAR10DataModule\",\n",
    "        \"data_dir\": data_dir,\n",
    "        \"batch_size\": 320,\n",
    "        \"num_workers\": 4,\n",
    "        \"pin_memory\": True,\n",
    "        \"shuffle\": True,\n",
    "        \"trainset_data_aug\": False,\n",
    "        # This is the irreducible loss model training, so we train on the\n",
    "        # holdout set (we call this set the \"valset\" in the global terminology for the dataset\n",
    "        # splits). Thus, we need augmentation on the valset\n",
    "        \"valset_data_aug\": True,\n",
    "    },\n",
    "    \"callbacks\": {\n",
    "        # We want to save that irreducible loss model with the lowest validation\n",
    "        # loss (we validate on the \"trainset\", in global terminology for the\n",
    "        # dataset splits).\n",
    "        \"model_checkpoint\": {\n",
    "            \"_target_\": \"pytorch_lightning.callbacks.ModelCheckpoint\",\n",
    "            \"monitor\": \"val_loss_epoch\",\n",
    "            \"mode\": \"min\",\n",
    "            \"save_top_k\": 1,\n",
    "            \"save_last\": True,\n",
    "            \"verbose\": False,\n",
    "            \"dirpath\": os.path.join(\"tutorial_outputs\", \"irreducible_loss_model\"),\n",
    "            \"filename\": \"epoch_{epoch:03d}\",\n",
    "            \"auto_insert_metric_name\": False,\n",
    "        },\n",
    "    },\n",
    "    \"logger\": {\n",
    "        # Log with wandb, you could choose a different logger\n",
    "        \"wandb\": {\n",
    "            \"_target_\": \"pytorch_lightning.loggers.wandb.WandbLogger\",\n",
    "            \"project\": my_project,\n",
    "            \"save_dir\": \".\",\n",
    "            \"entity\": my_entity,\n",
    "            \"job_type\": \"train\",\n",
    "        }\n",
    "    },\n",
    "    \"seed\": 12,\n",
    "    \"debug\": False,\n",
    "    \"ignore_warnings\": True,\n",
    "    \"test_after_training\": True,\n",
    "    \"base_outdir\": \"logs\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert config to OmegaConf structured dict (default for Hydra), and pretty-print\n",
    "config = OmegaConf.create(config)\n",
    "utils.print_config(\n",
    "    config,\n",
    "    fields=(\n",
    "        \"trainer\",\n",
    "        \"model\",\n",
    "        \"datamodule\",\n",
    "        \"callbacks\",\n",
    "        \"logger\",\n",
    "        \"seed\",\n",
    "        \"optimizer\",\n",
    "        \"scheduler\",\n",
    "    ),\n",
    "    resolve=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We have split all datasets into a \"trainset\", a \"valset\", and a \"testset\".\n",
    "* The \"valset\" is what we call the \"holdout set\" in the paper; it is used for training of the irreducible loss model.\n",
    "* The \"trainset\" is what we call the training set in the paper. It is used for training the target model, and as a validation set for the irreducible loss model, to find the epoch with the lowest loss in irreducible loss model training.\n",
    "* The \"testset\" is used to evaluate target model performance.\n",
    "\n",
    "(For earlier experiments, we used the \"valset\" as the validation set for the target model. However, this was rarely used, as we did not tune any hyperparameters of the target model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for random number generators in pytorch, numpy and python.random\n",
    "if \"seed\" in config:\n",
    "    seed_everything(config.seed, workers=True)\n",
    "\n",
    "# Init lightning datamodule\n",
    "print(f\"Instantiating datamodule <{config.datamodule._target_}>\")\n",
    "datamodule: LightningDataModule = hydra.utils.instantiate(config.datamodule)\n",
    "datamodule.setup()\n",
    "\n",
    "# Init lightning model\n",
    "print(f\"Instantiating model <{config.model._target_}>\")\n",
    "pl_model: LightningModule = hydra.utils.instantiate(\n",
    "    config=config.model,\n",
    "    optimizer_config=utils.mask_config(\n",
    "        config.get(\"optimizer\", None)\n",
    "    ),  # When initialising the optimiser, you need to pass it the model parameters. As we haven't initialised the model yet, we cannot initialise the optimizer here. Thus, we need to pass-through the optimizer-config, to initialise it later. However, hydra.utils.instantiate will instatiate everything that looks like a config (if _recursive_==True, which is required here bc OneModel expects a model argument). Thus, we \"mask\" the optimizer config from hydra, by modifying the dict so that hydra no longer recognises it as a config.\n",
    "    scheduler_config=utils.mask_config(\n",
    "        config.get(\"scheduler\", None)\n",
    "    ),  # see line above\n",
    "    datamodule=datamodule,\n",
    "    _convert_=\"partial\",\n",
    ")\n",
    "\n",
    "# Init lightning callbacks. Here, we only use one callback: saving the model\n",
    "# with the lowest validation set loss. \n",
    "callbacks: List[Callback] = []\n",
    "if \"callbacks\" in config:\n",
    "    for _, cb_conf in config.callbacks.items():\n",
    "        if \"_target_\" in cb_conf:\n",
    "            print(f\"Instantiating callback <{cb_conf._target_}>\")\n",
    "            callbacks.append(hydra.utils.instantiate(cb_conf))\n",
    "\n",
    "# Init lightning loggers. Here, we use wandb.\n",
    "logger: List[LightningLoggerBase] = []\n",
    "if \"logger\" in config:\n",
    "    for _, lg_conf in config.logger.items():\n",
    "        if \"_target_\" in lg_conf:\n",
    "            print(f\"Instantiating logger <{lg_conf._target_}>\")\n",
    "            logger.append(hydra.utils.instantiate(lg_conf))\n",
    "\n",
    "# Init lightning trainer\n",
    "print(f\"Instantiating trainer <{config.trainer._target_}>\")\n",
    "trainer: Trainer = hydra.utils.instantiate(\n",
    "    config.trainer, callbacks=callbacks, logger=logger, _convert_=\"partial\"\n",
    ")\n",
    "\n",
    "# Send config to all lightning loggers\n",
    "print(\"Logging hyperparameters!\")\n",
    "trainer.logger.log_hyperparams(config)\n",
    "\n",
    "# Train the model.\n",
    "print(\"Starting training!\")\n",
    "trainer.fit(\n",
    "    pl_model,\n",
    "    train_dataloaders=datamodule.val_dataloader(), # see Markdown comment above\n",
    "    val_dataloaders=datamodule.train_dataloader(), # see Markdown comment above\n",
    ")\n",
    "\n",
    "# Evaluate model on test set, using the best model achieved during training\n",
    "if config.get(\"test_after_training\") and not config.trainer.get(\"fast_dev_run\"):\n",
    "    print(\"Starting testing!\")\n",
    "    trainer.test(test_dataloaders=datamodule.test_dataloader())\n",
    "\n",
    "def evaluate_and_save_model_from_checkpoint_path(checkpoint_path, name):\n",
    "    \"\"\"Compute irreducible loss for the whole trainset with the best model\"\"\"\n",
    "\n",
    "    # load best model\n",
    "    model = OneModel.load_from_checkpoint(checkpoint_path)\n",
    "    \n",
    "    # compute irreducible losses\n",
    "    model.eval()\n",
    "    irreducible_loss_and_checks = utils.compute_losses_with_sanity_checks(\n",
    "        dataloader=datamodule.train_dataloader(), model=model\n",
    "    )\n",
    "\n",
    "    # save irred losses in same directory as model checkpoint\n",
    "    path = os.path.join(\n",
    "        os.path.dirname(trainer.checkpoint_callback.best_model_path),\n",
    "        name,\n",
    "    )\n",
    "    torch.save(irreducible_loss_and_checks, path)\n",
    "\n",
    "    return path\n",
    "\n",
    "saved_path = evaluate_and_save_model_from_checkpoint_path(\n",
    "    trainer.checkpoint_callback.best_model_path, \"irred_losses_and_checks.pt\"\n",
    ")\n",
    "\n",
    "print(f\"Using monitor: {trainer.checkpoint_callback.monitor}\")\n",
    "\n",
    "# Print path to best checkpoint\n",
    "print(f\"Best checkpoint path:\\n{trainer.checkpoint_callback.best_model_path}\")\n",
    "print(f\"Best checkpoint irred_losses_path:\\n{saved_path}\")\n",
    "\n",
    "\n",
    "# Make sure everything closed properly\n",
    "log.info(\"Finalizing!\")\n",
    "utils.finish(\n",
    "    config=config,\n",
    "    model=pl_model,\n",
    "    datamodule=datamodule,\n",
    "    trainer=trainer,\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to download the datasets\n",
    "data_dir = \"/path/to/dir/\"\n",
    "\n",
    "# where to upload the weights and biases logs\n",
    "my_project = \"tutorial_notebook\"\n",
    "my_entity = \"xyz\"\n",
    "\n",
    "# You could choose any selection method here\n",
    "# (\"src.curricula.selection_methods.xyz\"). We have implemented:\n",
    "# reducible_loss_selection, uniform_selection, ce_loss_selection,\n",
    "# irreducible_loss_selection, gradnorm_ub_selection, and thers\n",
    "selection_method = \"reducible_loss_selection\"\n",
    "\n",
    "# Path to irreducible losses. Transferred from irreducible loss model training.\n",
    "# You can replace this with the path if you want to run target model training\n",
    "# without rerunning irreducible loss model training.\n",
    "path_to_irreducible_losses = saved_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model\": {\n",
    "        \"_target_\": \"src.models.MultiModels.MultiModels\",\n",
    "        \"large_model\": {\n",
    "            \"_target_\": \"src.models.modules.resnet_cifar.ResNet18\"\n",
    "        },\n",
    "        \"percent_train\": 0.1,\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"_target_\": \"torch.optim.AdamW\",\n",
    "        \"lr\": 0.001\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"_target_\": \"pytorch_lightning.Trainer\",\n",
    "        \"gpus\": 1,\n",
    "        \"min_epochs\": 1,\n",
    "        \"max_epochs\": 175,\n",
    "        \"weights_summary\": None,\n",
    "        \"progress_bar_refresh_rate\": 20,\n",
    "    },\n",
    "    \"datamodule\": {\n",
    "        \"_target_\": \"src.datamodules.datamodules.CIFAR10DataModule\",\n",
    "        \"data_dir\": data_dir,\n",
    "        \"batch_size\": 320,\n",
    "        \"num_workers\": 4,\n",
    "        \"pin_memory\": True,\n",
    "        \"shuffle\": True,\n",
    "        \"trainset_data_aug\": True,\n",
    "        \"valset_data_aug\": False,\n",
    "    },\n",
    "    \"selection_method\": {\n",
    "        \"_target_\": \"src.curricula.selection_methods.\" + selection_method\n",
    "    },\n",
    "    \"callbacks\": {\n",
    "        \"model_checkpoint\": {\n",
    "            \"_target_\": \"pytorch_lightning.callbacks.ModelCheckpoint\",\n",
    "            \"monitor\": \"val_acc_epoch\",\n",
    "            \"mode\": \"max\",\n",
    "            \"save_top_k\": 1,\n",
    "            \"save_last\": True,\n",
    "            \"verbose\": False,\n",
    "            \"dirpath\": os.path.join(\"tutorial_outputs\", \"target_model\"),\n",
    "            \"filename\": \"epoch_{epoch:03d}\",\n",
    "            \"auto_insert_metric_name\": False,\n",
    "        },\n",
    "    },\n",
    "    \"logger\": {\n",
    "        \"wandb\": {\n",
    "            \"_target_\": \"pytorch_lightning.loggers.wandb.WandbLogger\",\n",
    "            \"project\": my_project,\n",
    "            \"save_dir\": \".\",\n",
    "            \"entity\": my_entity,\n",
    "            \"job_type\": \"train\",\n",
    "        }\n",
    "    },\n",
    "    \"irreducible_loss_generator\": {\n",
    "        \"_target_\": \"torch.load\",\n",
    "        \"f\": path_to_irreducible_losses,\n",
    "    },\n",
    "\n",
    "\n",
    "    \"debug\": False,\n",
    "    \"ignore_warnings\": True,\n",
    "    \"test_after_training\": True,\n",
    "    \"seed\": 12,\n",
    "    \"eval_set\": \"val\", # set to test if you want to evaluate on the test set\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert config to OmegaConf structured dict (default for Hydra), and pretty-print\n",
    "config = OmegaConf.create(config)\n",
    "utils.print_config(\n",
    "    config,\n",
    "    fields=(\n",
    "        \"trainer\",\n",
    "        \"selection_method\",\n",
    "        \"model\",\n",
    "        \"irreducible_loss_generator\",\n",
    "        \"datamodule\",\n",
    "        \"callbacks\",\n",
    "        \"logger\",\n",
    "        \"seed\",\n",
    "        \"optimizer\",\n",
    "    ),\n",
    "    resolve=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for random number generators in pytorch, numpy and python.random\n",
    "if \"seed\" in config:\n",
    "    seed_everything(config.seed, workers=True)\n",
    "\n",
    "# init irreducible loss generator (precomputed losses, or irreducible loss\n",
    "# model)\n",
    "irreducible_loss_generator = hydra.utils.instantiate(\n",
    "    config.irreducible_loss_generator\n",
    ")\n",
    "\n",
    "# If precomputed losses are used, verify that the sorting\n",
    "# of the precomputes losses matches the dataset\n",
    "if type(irreducible_loss_generator) is dict:\n",
    "    # instantiate a separate datamodule, so that the main datamodule is\n",
    "    # instantiated with the same random seed whether or not the precomputed\n",
    "    # losses are used\n",
    "    datamodule_temp = hydra.utils.instantiate(config.datamodule)\n",
    "    datamodule_temp.setup()\n",
    "    utils.verify_correct_dataset_order(\n",
    "        dataloader=datamodule_temp.train_dataloader(),\n",
    "        sorted_target=irreducible_loss_generator[\"sorted_targets\"],\n",
    "        idx_of_control_images=irreducible_loss_generator[\"idx_of_control_images\"],\n",
    "        control_images=irreducible_loss_generator[\"control_images\"],\n",
    "        dont_compare_control_images=config.datamodule.get(\n",
    "            \"trainset_data_aug\", False\n",
    "        ),  # cannot compare images from irreducible loss model training run with those of the current run if there is trainset augmentation\n",
    "    )\n",
    "\n",
    "    del datamodule_temp\n",
    "\n",
    "    irreducible_loss_generator = irreducible_loss_generator[\"irreducible_losses\"]\n",
    "\n",
    "    # Set seed again, so that the main datamodule is instantiated with the\n",
    "    # same random seed whether or not the precomputed losses are used\n",
    "    if \"seed\" in config:\n",
    "        seed_everything(config.seed, workers=True)\n",
    "\n",
    "# Init lightning datamodule\n",
    "print(f\"Instantiating datamodule <{config.datamodule._target_}>\")\n",
    "datamodule: LightningDataModule = hydra.utils.instantiate(config.datamodule)\n",
    "datamodule.setup()\n",
    "\n",
    "# init selection method\n",
    "print(f\"Instantiating selection method <{config.selection_method._target_}>\")\n",
    "selection_method = hydra.utils.instantiate(config.selection_method)\n",
    "\n",
    "# Init lightning model\n",
    "print(f\"Instantiating models\")\n",
    "pl_model: LightningModule = hydra.utils.instantiate(\n",
    "    config.model,\n",
    "    selection_method=selection_method,\n",
    "    irreducible_loss_generator=irreducible_loss_generator,\n",
    "    datamodule=datamodule,\n",
    "    optimizer_config=utils.mask_config(\n",
    "        config.get(\"optimizer\", None)\n",
    "    ),  # When initialising the optimiser, you need to pass it the model parameters. As we haven't initialised the model yet, we cannot initialise the optimizer here. Thus, we need to pass-through the optimizer-config, to initialise it later. However, hydra.utils.instantiate will instatiate everything that looks like a config (if _recursive_==True, which is required here bc OneModel expects a model argument). Thus, we \"mask\" the optimizer config from hydra, by modifying the dict so that hydra no longer recognises it as a config.\n",
    "    _convert_=\"partial\",\n",
    ")\n",
    "\n",
    "# Init lightning callbacks\n",
    "callbacks: List[Callback] = []\n",
    "if \"callbacks\" in config:\n",
    "    for _, cb_conf in config.callbacks.items():\n",
    "        if \"_target_\" in cb_conf:\n",
    "            print(f\"Instantiating callback <{cb_conf._target_}>\")\n",
    "            callbacks.append(hydra.utils.instantiate(cb_conf))\n",
    "\n",
    "# Init lightning loggers\n",
    "logger: List[LightningLoggerBase] = []\n",
    "if \"logger\" in config:\n",
    "    for _, lg_conf in config.logger.items():\n",
    "        if \"_target_\" in lg_conf:\n",
    "            log.info(f\"Instantiating logger <{lg_conf._target_}>\")\n",
    "            logger.append(hydra.utils.instantiate(lg_conf))\n",
    "\n",
    "# Init lightning trainer\n",
    "print(f\"Instantiating trainer <{config.trainer._target_}>\")\n",
    "trainer: Trainer = hydra.utils.instantiate(\n",
    "    config.trainer, callbacks=callbacks, logger=logger, _convert_=\"partial\"\n",
    ")\n",
    "\n",
    "# Send config to all lightning loggers\n",
    "print(\"Logging hyperparameters!\")\n",
    "trainer.logger.log_hyperparams(config)\n",
    "\n",
    "# create eval set\n",
    "if config.eval_set == \"val\":\n",
    "    val_dataloader = datamodule.val_dataloader()\n",
    "elif config.eval_set == \"test\":\n",
    "    val_dataloader = datamodule.test_dataloader()\n",
    "    print(\n",
    "        \"Using the test set as the validation dataloader. This is for final figures in the paper\"\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training!\")\n",
    "trainer.fit(\n",
    "    pl_model,\n",
    "    train_dataloaders=datamodule.train_dataloader(),\n",
    "    val_dataloaders=val_dataloader, # we pass the eval set as the validation set to trainer.fit because we want to know the eval set accuracy after each epoch\n",
    ")\n",
    "\n",
    "# Make sure everything closed properly\n",
    "print(\"Finalizing!\")\n",
    "utils.finish(\n",
    "    config=config,\n",
    "    model=pl_model,\n",
    "    datamodule=datamodule,\n",
    "    trainer=trainer,\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# Print path to best checkpoint\n",
    "print(f\"Best checkpoint path:\\n{trainer.checkpoint_callback.best_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfd0ac69e947a4c3f41d3e96b87afe4576cfb36da8142bbbf50a4fd2ea8d9d14"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
