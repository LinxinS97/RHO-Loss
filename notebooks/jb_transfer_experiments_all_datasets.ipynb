{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "223fbcee-c1be-4bf7-8a77-2aef9d343df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "\n",
    "import functools\n",
    "import itertools\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams[\"font.family\"] = \"Times\"\n",
    "plt.rcParams[\"font.weight\"] = \"light\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23fe9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_merge(path, filters, keys, verbose=True):\n",
    "    # get experiments\n",
    "    api = wandb.Api()\n",
    "    runs = api.runs(path=path, filters=filters) # filters so loading doesn't take too much time\n",
    "    print(\"number of runs: \", len(runs))\n",
    "\n",
    "    ## loading block suggested by wandb, somewhat modified\n",
    "    summary_list, config_list, name_list, history_list = [], [], [], []\n",
    "    for idx, run in enumerate(runs): \n",
    "        # .summary contains the output keys/values for metrics like accuracy.\n",
    "        #  We call ._json_dict to omit large files \n",
    "        summary_list.append(run.summary._json_dict)\n",
    "        # .config contains the hyperparameters.\n",
    "        #  We remove special values that start with _.\n",
    "        config = {k: v for k,v in run.config.items()\n",
    "            if not k.startswith('_')}\n",
    "        config[\"id4ana\"] = idx\n",
    "        # .name is the human-readable name of the run.\n",
    "        config[\"name\"] = run.name\n",
    "        config_list.append(config)\n",
    "\n",
    "        history = run.history(keys=keys) #filter so that loading doesn't take too long\n",
    "        history[\"id4ana\"] = idx\n",
    "        history_list.append(history)\n",
    "\n",
    "    ## create config columns\n",
    "    config_df = pd.DataFrame(config_list) # pandas is amazing, lol\n",
    "\n",
    "    # look at it\n",
    "    if verbose:\n",
    "        print(\"config table: \\n\")\n",
    "        print(config_df.head())\n",
    "        print(config_df.info())\n",
    "\n",
    "    #create history_df\n",
    "    history_df = pd.concat(history_list)\n",
    "    if verbose:\n",
    "        print(\"history table: \\n\")\n",
    "        print(history_df)\n",
    "\n",
    "    #merge\n",
    "    df = pd.merge(config_df, history_df, on=\"id4ana\")\n",
    "    if verbose:\n",
    "        print(\"final df: \\n\")\n",
    "        print(df)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def compute_speedups(df, reference_epoch):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        df: dataframe, which contains exactly 2 experiment conditions (which potentially multiple seeds per condition, and redloss can have additional seeds over uniform, but not the other way around):\n",
    "            all uniform seeds\n",
    "            all seeds of redloss, to be compared against uniform. (does not accept different redloss runs, e.g. with different IrLoMos)\n",
    "        reference_epoch, int: will compare redloss against the maximum accuracy that uniform has reached by that epoch.\n",
    "    Outputs:\n",
    "        speedups: list of speedups, one for each seed. If redloss never reaches the acc of uniform at the target epoch, the speedup is 0\n",
    "    \"\"\"\n",
    "    df = df.sort_values(\"epoch\")\n",
    "    seeds = df[df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\"][\"seed\"].unique()\n",
    "    unif_seeds = df[df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.uniform_selection\"][\"seed\"].unique()\n",
    "    if len(seeds) == 0:\n",
    "        print(\"no redloss runs, can't compute speedup\")\n",
    "        return [np.nan]\n",
    "    if len(unif_seeds) == 0:\n",
    "        print(\"no uniform runs, can't compute speedup\")\n",
    "        return [np.nan]\n",
    "    speedups = []\n",
    "    for seed in seeds:\n",
    "        seed_df = df[df[\"seed\"] == seed]\n",
    "        if len(seed_df[\"selection_method/_target_\"].unique()) == 2: # both selection methods have this seed\n",
    "            unif_df = seed_df[(seed_df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.uniform_selection\")]\n",
    "            redloss_df = seed_df[(seed_df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\")]\n",
    "            speedup = _compute_speedup(unif_df, redloss_df, reference_epoch)\n",
    "        elif (len(seed_df[\"selection_method/_target_\"].unique()) == 1): # only redloss method has this seed, take uniform from another seed\n",
    "            unif_seed_df = df[df[\"seed\"] == unif_seeds[0]]\n",
    "            unif_df = unif_seed_df[(unif_seed_df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.uniform_selection\")]\n",
    "            redloss_df = seed_df[(seed_df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\")]\n",
    "            speedup = _compute_speedup(unif_df, redloss_df, reference_epoch)\n",
    "        else:\n",
    "            print(\"this probably shouldn't happen\")\n",
    "            speedup = np.nan        \n",
    "        speedups.append(speedup)\n",
    "    return speedups\n",
    "\n",
    "def _compute_speedup(unif_df, redloss_df, reference_epoch):\n",
    "    unif_df[\"cummax_val_acc_epoch\"] = unif_df.cummax()[\"val_acc_epoch\"]\n",
    "    unif_acc_at_reference = unif_df.loc[unif_df[\"epoch\"] == reference_epoch, \"cummax_val_acc_epoch\"].iloc[0]\n",
    "    if redloss_df[\"val_acc_epoch\"].max() >= unif_acc_at_reference:\n",
    "        rows_at_which_redloss_greater_unif_acc_at_reference = (redloss_df[\"val_acc_epoch\"] >= unif_acc_at_reference)\n",
    "        epoch_at_which_redloss_greater_unif_acc_at_reference = redloss_df[rows_at_which_redloss_greater_unif_acc_at_reference].iloc[0][\"epoch\"]\n",
    "        speedup = reference_epoch/ epoch_at_which_redloss_greater_unif_acc_at_reference\n",
    "    else:\n",
    "        speedup = 0\n",
    "    return speedup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010ec432",
   "metadata": {},
   "source": [
    "# Loading, merging, and saving all dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24462f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "save_dir_dfs = \"dfs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5a162",
   "metadata": {},
   "source": [
    "## CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79f78908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of runs:  86\n",
      "number of runs:  48\n",
      "number of runs:  18\n",
      "number of runs:  6\n",
      "number of runs:  6\n",
      "number of runs:  6\n"
     ]
    }
   ],
   "source": [
    "#####----- CIFAR10 -----#####\n",
    "### hyperparameter transfer experiments\n",
    "path = \"goldiprox/jb_cifar10\"\n",
    "filters = {\"tags\": \"exp_vary_target_hypers\"}\n",
    "keys = [\"trainer/global_step\", \"val_acc_epoch\", \"epoch\"]\n",
    "\n",
    "df = download_and_merge(path, filters, keys, verbose=False)\n",
    "\n",
    "# replace any nans\n",
    "df.loc[df[\"optimizer/weight_decay\"].isna(), \"optimizer/weight_decay\"] = 0.01\n",
    "weight_decays = df[\"optimizer/weight_decay\"].unique()\n",
    "\n",
    "# remove runs\n",
    "df = df[df[\"seed\"]==12]\n",
    "\n",
    "# save\n",
    "exp_name = \"CIFAR10_hypers\"\n",
    "dfs[exp_name] = df\n",
    "with open(os.path.join(save_dir_dfs, exp_name + \"_df.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "\n",
    "### architecture transfer experiments\n",
    "path = \"goldiprox/jb_cifar10\"\n",
    "filters = {\"tags\": \"exp_vary_target_arch\"}\n",
    "keys = [\"trainer/global_step\", \"val_acc_epoch\", \"epoch\"]\n",
    "\n",
    "df = download_and_merge(path, filters, keys, verbose=False)\n",
    "\n",
    "# save\n",
    "exp_name = \"CIFAR10_archs\"\n",
    "dfs[exp_name] = df\n",
    "with open(os.path.join(save_dir_dfs, exp_name + \"_df.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "\n",
    "### holdout set ablation experiments\n",
    "path = \"goldiprox/jb_cifar10\"\n",
    "filters = {\"tags\": \"fraction_valset_exp\"}\n",
    "keys = [\"trainer/global_step\", \"val_acc_epoch\", \"epoch\"]\n",
    "\n",
    "df = download_and_merge(path, filters, keys, verbose=False)\n",
    "\n",
    "# save\n",
    "exp_name = \"CIFAR10_holdout_set\"\n",
    "dfs[exp_name] = df\n",
    "with open(os.path.join(save_dir_dfs, exp_name + \"_df.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "\n",
    "### double IrLoMo experiments\n",
    "path = \"goldiprox/jb_cifar10\"\n",
    "filters = {\"tags\": \"double_irlomo\"}\n",
    "keys = [\"trainer/global_step\", \"val_acc_epoch\", \"epoch\"]\n",
    "\n",
    "df = download_and_merge(path, filters, keys, verbose=False)\n",
    "\n",
    "# save\n",
    "exp_name = \"CIFAR10_double_IrLoMo\"\n",
    "dfs[exp_name] = df\n",
    "with open(os.path.join(save_dir_dfs, exp_name + \"_df.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "\n",
    "### small CNN\n",
    "path = \"goldiprox/jb_cifar10\"\n",
    "filters = {\"tags\": \"IrLoMo_ablation_4_ICLR_revision\"}\n",
    "keys = [\"trainer/global_step\", \"val_acc_epoch\", \"epoch\"]\n",
    "\n",
    "df = download_and_merge(path, filters, keys, verbose=False)\n",
    "\n",
    "# save\n",
    "exp_name = \"CIFAR10_small_CNN\"\n",
    "dfs[exp_name] = df\n",
    "with open(os.path.join(save_dir_dfs, exp_name + \"_df.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "\n",
    "### default\n",
    "path = \"goldiprox/jb_cifar10\"\n",
    "filters = {\"tags\": \"default_values_4_ICLR_revision\"}\n",
    "keys = [\"trainer/global_step\", \"val_acc_epoch\", \"epoch\"]\n",
    "\n",
    "df = download_and_merge(path, filters, keys, verbose=False)\n",
    "\n",
    "# save\n",
    "exp_name = \"CIFAR10_default\"\n",
    "dfs[exp_name] = df\n",
    "with open(os.path.join(save_dir_dfs, exp_name + \"_df.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e11fa9",
   "metadata": {},
   "source": [
    "## CINIC-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b463be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of runs:  72\n",
      "number of runs:  46\n",
      "number of runs:  44\n",
      "number of runs:  44\n",
      "number of runs:  44\n",
      "number of runs:  44\n"
     ]
    }
   ],
   "source": [
    "#####----- CINIC10 -----#####\n",
    "### hyperparameter transfer experiments\n",
    "path = \"goldiprox/cinic10_hyperparam_transfer\"\n",
    "filters = None\n",
    "keys = [\"trainer/global_step\", \"val_acc_epoch\", \"epoch\"]\n",
    "\n",
    "df = download_and_merge(path, filters, keys, verbose=False)\n",
    "\n",
    "# replace any nans\n",
    "df.loc[df[\"optimizer/weight_decay\"].isna(), \"optimizer/weight_decay\"] = 0.01\n",
    "weight_decays = df[\"optimizer/weight_decay\"].unique()\n",
    "\n",
    "# remove runs\n",
    "df = df[df[\"seed\"]==1]\n",
    "\n",
    "# save\n",
    "exp_name = \"CINIC10_hypers\"\n",
    "dfs[exp_name] = df\n",
    "with open(os.path.join(save_dir_dfs, exp_name + \"_df.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "\n",
    "### architecture transfer experiments\n",
    "path = \"goldiprox/cinic_transfer\"\n",
    "filters = {\"tags\": \"exp_vary_target_arch\"}\n",
    "keys = [\"trainer/global_step\", \"val_acc_epoch\", \"epoch\"]\n",
    "\n",
    "df = download_and_merge(path, filters, keys, verbose=False)\n",
    "\n",
    "# save\n",
    "exp_name = \"CINIC10_archs\"\n",
    "dfs[exp_name] = df\n",
    "with open(os.path.join(save_dir_dfs, exp_name + \"_df.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "\n",
    "### holdout set ablation experiments\n",
    "path = \"goldiprox/cinic10_holdout_ablation\"\n",
    "filters = None\n",
    "keys = [\"trainer/global_step\", \"val_acc_epoch\", \"epoch\"]\n",
    "\n",
    "df = download_and_merge(path, filters, keys, verbose=False)\n",
    "\n",
    "# filter out irlomo training runs\n",
    "df = df[~df[\"irreducible_loss_generator/f\"].isna()]\n",
    "\n",
    "# save\n",
    "exp_name = \"CINIC10_holdout_set\"\n",
    "dfs[exp_name] = df\n",
    "with open(os.path.join(save_dir_dfs, exp_name + \"_df.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "\n",
    "### double IrLoMo experiments\n",
    "path = \"goldiprox/cinic10_holdout_ablation\"\n",
    "filters = None\n",
    "keys = [\"trainer/global_step\", \"val_acc_epoch\", \"epoch\"]\n",
    "\n",
    "df = download_and_merge(path, filters, keys, verbose=False)\n",
    "\n",
    "# save\n",
    "exp_name = \"CINIC10_double_IrLoMo\"\n",
    "dfs[exp_name] = df\n",
    "with open(os.path.join(save_dir_dfs, exp_name + \"_df.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "\n",
    "### small CNN\n",
    "path = \"goldiprox/cinic10_holdout_ablation\"\n",
    "filters = None\n",
    "keys = [\"trainer/global_step\", \"val_acc_epoch\", \"epoch\"]\n",
    "\n",
    "df = download_and_merge(path, filters, keys, verbose=False)\n",
    "\n",
    "# save\n",
    "exp_name = \"CINIC10_small_CNN\"\n",
    "dfs[exp_name] = df\n",
    "with open(os.path.join(save_dir_dfs, exp_name + \"_df.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "\n",
    "### default\n",
    "path = \"goldiprox/cinic10_holdout_ablation\"\n",
    "filters = None\n",
    "keys = [\"trainer/global_step\", \"val_acc_epoch\", \"epoch\"]\n",
    "\n",
    "df = download_and_merge(path, filters, keys, verbose=False)\n",
    "\n",
    "# save\n",
    "exp_name = \"CINIC10_default\"\n",
    "dfs[exp_name] = df\n",
    "with open(os.path.join(save_dir_dfs, exp_name + \"_df.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202baeec",
   "metadata": {},
   "source": [
    "## CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a977b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of runs:  77\n",
      "number of runs:  51\n",
      "number of runs:  22\n",
      "number of runs:  9\n",
      "number of runs:  22\n",
      "number of runs:  22\n"
     ]
    }
   ],
   "source": [
    "#####---- CIFAR100 -----#####\n",
    "### hyperparameter transfer experiments\n",
    "path = \"goldiprox/cifar100_hyper_ablation\"\n",
    "filters = None\n",
    "keys = [\"trainer/global_step\", \"val_acc_epoch\", \"epoch\"]\n",
    "\n",
    "df = download_and_merge(path, filters, keys, verbose=False)\n",
    "\n",
    "# replace any nans\n",
    "df.loc[df[\"optimizer/weight_decay\"].isna(), \"optimizer/weight_decay\"] = 0.01\n",
    "\n",
    "# remove runs\n",
    "df = df[df[\"seed\"]==12]\n",
    "\n",
    "# save\n",
    "exp_name = \"CIFAR100_hypers\"\n",
    "dfs[exp_name] = df\n",
    "with open(os.path.join(save_dir_dfs, exp_name + \"_df.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "\n",
    "### architecture transfer experiments\n",
    "path = \"goldiprox/cifar100_transfer_ablations\"\n",
    "filters = None\n",
    "keys = [\"trainer/global_step\", \"val_acc_epoch\", \"epoch\"]\n",
    "\n",
    "df = download_and_merge(path, filters, keys, verbose=False)\n",
    "\n",
    "# filter out IrLoMo training runs\n",
    "df = df[~df[\"model/large_model/_target_\"].isna()]\n",
    "\n",
    "# filter on the correct eval set, but not available for two architectures\n",
    "models_w_valset_runs_only = [\"src.models.modules.cifar_model_zoo.inception.inception_v3\", \"src.models.modules.cifar_model_zoo.googlenet.googlenet\"]\n",
    "df = df[(df[\"eval_set\"] == \"test\") | df[\"model/large_model/_target_\"].isin(models_w_valset_runs_only)]\n",
    "\n",
    "# save\n",
    "exp_name = \"CIFAR100_archs\"\n",
    "dfs[exp_name] = df\n",
    "with open(os.path.join(save_dir_dfs, exp_name + \"_df.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "\n",
    "### holdout set ablation experiments\n",
    "path = \"goldiprox/cifar100_holdout_ablations\"\n",
    "filters = None\n",
    "keys = [\"trainer/global_step\", \"val_acc_epoch\", \"epoch\"]\n",
    "\n",
    "df = download_and_merge(path, filters, keys, verbose=False)\n",
    "\n",
    "# filter out potential IrLoMo training runs\n",
    "df = df[~df[\"irreducible_loss_generator/checkpoint_path\"].isna()]\n",
    "\n",
    "# save\n",
    "exp_name = \"CIFAR100_holdout_set\"\n",
    "dfs[exp_name] = df\n",
    "with open(os.path.join(save_dir_dfs, exp_name + \"_df.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "\n",
    "### double IrLoMo experiments\n",
    "path = \"goldiprox/cifar100_double_ablation\"\n",
    "filters = {\"tags\": \"double_irlomo_figure_for_ICLR_revision\"}\n",
    "keys = [\"trainer/global_step\", \"val_acc_epoch\", \"epoch\"]\n",
    "\n",
    "df = download_and_merge(path, filters, keys, verbose=False)\n",
    "\n",
    "# save\n",
    "exp_name = \"CIFAR100_double_IrLoMo\"\n",
    "dfs[exp_name] = df\n",
    "with open(os.path.join(save_dir_dfs, exp_name + \"_df.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "\n",
    "### small CNN\n",
    "path = \"goldiprox/cifar100_holdout_ablations\"\n",
    "filters = None\n",
    "keys = [\"trainer/global_step\", \"val_acc_epoch\", \"epoch\"]\n",
    "\n",
    "df = download_and_merge(path, filters, keys, verbose=False)\n",
    "\n",
    "# filter out potential IrLoMo training runs\n",
    "df = df[~df[\"irreducible_loss_generator/checkpoint_path\"].isna()]\n",
    "\n",
    "# save\n",
    "exp_name = \"CIFAR100_small_CNN\"\n",
    "dfs[exp_name] = df\n",
    "with open(os.path.join(save_dir_dfs, exp_name + \"_df.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "\n",
    "### default\n",
    "path = \"goldiprox/cifar100_holdout_ablations\"\n",
    "filters = None\n",
    "keys = [\"trainer/global_step\", \"val_acc_epoch\", \"epoch\"]\n",
    "\n",
    "df = download_and_merge(path, filters, keys, verbose=False)\n",
    "\n",
    "# filter out potential IrLoMo training runs\n",
    "df = df[~df[\"irreducible_loss_generator/checkpoint_path\"].isna()]\n",
    "\n",
    "# save\n",
    "exp_name = \"CIFAR100_default\"\n",
    "dfs[exp_name] = df\n",
    "with open(os.path.join(save_dir_dfs, exp_name + \"_df.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a865f2",
   "metadata": {},
   "source": [
    "# Computing speedups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1ac3f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_speedups = {}\n",
    "save_dir_dfs = \"dfs\"\n",
    "save_dir_speedups = \"speedups\"\n",
    "\n",
    "def load_df(exp_name, path=save_dir_dfs):\n",
    "    try:\n",
    "        df = dfs[exp_name] #yes, yes, I probably shouldn't do it like this. Just don't call anything dfs, OK :-)\n",
    "    except:\n",
    "        with open(os.path.join(path, exp_name + \"_df.pkl\"), \"rb\") as f:\n",
    "            df = pickle.load(f)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dccf8f",
   "metadata": {},
   "source": [
    "## CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9fd850",
   "metadata": {},
   "source": [
    "### settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d341a0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch in uniform training in reference to which the speedups are computed\n",
    "cifar10_reference_epoch = 99\n",
    "\n",
    "# determines which irreducible loss model to use for computing the speedups.\n",
    "# Options are (\"redloss_w_resnet18\", \"redloss_w_sCNN\"). Does not influence\n",
    "# \"default\" or \"small_CNN\" experiments, of course (these are alwas Resnet18 and\n",
    "# small CNN, respectively)\n",
    "cifar10_which_irlomo = \"redloss_w_sCNN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffab2943",
   "metadata": {},
   "source": [
    "### computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "068453e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_298696/184083434.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unif_df[\"cummax_val_acc_epoch\"] = unif_df.cummax()[\"val_acc_epoch\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0.0001, 160, 0.001): [2.127659574468085], (0.0001, 160, 0.01): [2.0833333333333335], (0.0001, 160, 0.1): [2.127659574468085], (0.0001, 320, 0.001): [2.0833333333333335], (0.0001, 320, 0.01): [2.0], (0.0001, 320, 0.1): [2.0833333333333335], (0.0001, 960, 0.001): [1.5384615384615385], (0.0001, 960, 0.01): [1.5625], (0.0001, 960, 0.1): [1.639344262295082], (0.001, 160, 0.001): [2.857142857142857], (0.001, 160, 0.01): [2.857142857142857], (0.001, 160, 0.1): [3.4482758620689653], (0.001, 320, 0.001): [2.5641025641025643], (0.001, 320, 0.01): [2.857142857142857], (0.001, 320, 0.1): [3.225806451612903], (0.001, 960, 0.001): [1.7857142857142858], (0.001, 960, 0.01): [1.639344262295082], (0.001, 960, 0.1): [2.0], (0.01, 160, 0.001): [1.6129032258064515], (0.01, 160, 0.01): [2.0], (0.01, 160, 0.1): [2.3255813953488373], (0.01, 320, 0.001): [1.492537313432836], (0.01, 320, 0.01): [1.8181818181818181], (0.01, 320, 0.1): [2.0833333333333335], (0.01, 960, 0.001): [0.970873786407767], (0.01, 960, 0.01): [0.9900990099009901], (0.01, 960, 0.1): [1.1764705882352942]}\n",
      "{'src.models.modules.cifar_model_zoo.densenet.densenet121': [2.4390243902439024, 2.0833333333333335], 'src.models.modules.cifar_model_zoo.googlenet.googlenet': [2.4390243902439024, 2.7027027027027026], 'src.models.modules.cifar_model_zoo.inception.inception_v3': [2.2222222222222223, 2.0], 'src.models.modules.cifar_model_zoo.mobilenetv2.mobilenet_v2': [2.127659574468085, 2.0], 'src.models.modules.cifar_model_zoo.resnet.resnet18': [2.5641025641025643, 2.7777777777777777], 'src.models.modules.cifar_model_zoo.resnet.resnet34': [2.380952380952381, 2.3255813953488373], 'src.models.modules.cifar_model_zoo.resnet.resnet50': [2.127659574468085, 1.7857142857142858], 'src.models.modules.cifar_model_zoo.vgg.vgg11_bn': [0.8547008547008547, 0]}\n",
      "{'small CNN, 0.5': [2.7027027027027026, 2.7027027027027026], 'small CNN, 0.75': [3.0303030303030303, 2.857142857142857], 'small CNN, 0.25': [1.7543859649122806, 1.8867924528301887], 'small CNN, 0.33': [2.380952380952381, 2.272727272727273]}\n",
      "{'small CNN, double IrLoMo': [2.2222222222222223, 2.5]}\n",
      "{'small_cnn': [2.7777777777777777, 2.857142857142857, 2.7027027027027026]}\n",
      "{'default': [2.3255813953488373, 2.4390243902439024, 2.272727272727273]}\n"
     ]
    }
   ],
   "source": [
    "#####------------------------------#####\n",
    "exp_name = \"CIFAR10_hypers\"\n",
    "reference_epoch = cifar10_reference_epoch\n",
    "which_irlomo = cifar10_which_irlomo\n",
    "\n",
    "df = load_df(exp_name)\n",
    "\n",
    "conditions = {\n",
    "    \"uniform\": df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.uniform_selection\",\n",
    "    \"redloss_w_resnet18\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"] == \"/users/janner/goldiprox-hydra/outputs/2021-10-05/08-42-42-resnet-by-loss/checkpoints/irred_losses_and_checks.pt\"), \n",
    "    \"redloss_w_sCNN\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"] == \"/users/janner/goldiprox-hydra/outputs/2021-10-05/09-00-40-small-cnn-sel-4-loss/checkpoints/irred_losses_and_checks.pt\") \n",
    "}\n",
    "\n",
    "filtered = df[conditions[\"uniform\"] | conditions[which_irlomo]]\n",
    "grouped = filtered.groupby([\"optimizer/lr\", \"datamodule/batch_size\", \"optimizer/weight_decay\"])\n",
    "speedups = {}\n",
    "for name, group in grouped:\n",
    "    speedup = compute_speedups(group, reference_epoch)\n",
    "    speedups[name] = speedup\n",
    "\n",
    "print(speedups)\n",
    "with open(os.path.join(save_dir_speedups, exp_name + \".pkl\"), \"wb\") as f:\n",
    "    pickle.dump(speedups, f)\n",
    "\n",
    "all_speedups[exp_name] = speedups\n",
    "\n",
    "\n",
    "#####------------------------------#####\n",
    "exp_name = \"CIFAR10_archs\"\n",
    "reference_epoch = cifar10_reference_epoch\n",
    "which_irlomo = cifar10_which_irlomo\n",
    "\n",
    "df = load_df(exp_name)\n",
    "\n",
    "conditions = {\n",
    "    \"uniform\": df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.uniform_selection\",\n",
    "    \"redloss_w_resnet18\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"] == \"/users/janner/goldiprox-hydra/outputs/2021-10-05/08-42-42-resnet-by-loss/checkpoints/irred_losses_and_checks.pt\"), \n",
    "    \"redloss_w_sCNN\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"] == \"/users/janner/goldiprox-hydra/outputs/2021-10-05/09-00-40-small-cnn-sel-4-loss/checkpoints/irred_losses_and_checks.pt\") \n",
    "}\n",
    "\n",
    "filtered = df[conditions[\"uniform\"] | conditions[\"redloss_w_sCNN\"]]\n",
    "grouped = filtered.groupby([\"model/large_model/_target_\"])\n",
    "speedups = {}\n",
    "for name, group in grouped:\n",
    "    speedup = compute_speedups(group, reference_epoch)\n",
    "    speedups[name] = speedup\n",
    "\n",
    "print(speedups)\n",
    "with open(os.path.join(save_dir_speedups, exp_name + \".pkl\"), \"wb\") as f:\n",
    "    pickle.dump(speedups, f)\n",
    "\n",
    "all_speedups[exp_name] = speedups\n",
    "\n",
    "\n",
    "#####------------------------------#####\n",
    "exp_name = \"CIFAR10_holdout_set\"\n",
    "reference_epoch = cifar10_reference_epoch\n",
    "which_irlomo = cifar10_which_irlomo\n",
    "\n",
    "df = load_df(exp_name)\n",
    "\n",
    "irlomo_lookup_reverse = {\n",
    "    \"Resnet18, 0.75\": \"/users/janner/goldiprox-hydra/logs/runs/2021-11-16/14-40-48/checkpoints/irred_losses_and_checks.pt\",\n",
    "    \"Resnet18, 0.5\": \"/users/janner/goldiprox-hydra/logs/runs/2021-11-16/14-41-23/checkpoints/irred_losses_and_checks.pt\",\n",
    "    \"Resnet18, 0.33\": \"/users/janner/goldiprox-hydra/logs/runs/2021-11-16/14-40-35/checkpoints/irred_losses_and_checks.pt\",\n",
    "    \"Resnet18, 0.25\": \"/users/janner/goldiprox-hydra/logs/runs/2021-11-16/14-41-05/checkpoints/irred_losses_and_checks.pt\",\n",
    "    \"small CNN, 0.75\": \"/users/janner/goldiprox-hydra/logs/runs/2021-11-16/15-00-17/checkpoints/irred_losses_and_checks.pt\",\n",
    "    \"small CNN, 0.5\": \"/users/janner/goldiprox-hydra/logs/runs/2021-11-16/15-00-00/checkpoints/irred_losses_and_checks.pt\",\n",
    "    \"small CNN, 0.33\": \"/users/janner/goldiprox-hydra/logs/runs/2021-11-16/15-00-50/checkpoints/irred_losses_and_checks.pt\",\n",
    "    \"small CNN, 0.25\": \"/users/janner/goldiprox-hydra/logs/runs/2021-11-16/15-00-33/checkpoints/irred_losses_and_checks.pt\",\n",
    "}\n",
    "\n",
    "irlomo_lookup = {v: k for (k,v) in irlomo_lookup_reverse.items()}\n",
    "\n",
    "conditions = {\n",
    "    \"uniform\": df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.uniform_selection\",\n",
    "    \"redloss_w_resnet18\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"].isin([v for (k,v) in irlomo_lookup_reverse.items() if \"Resnet18\" in k])), \n",
    "    \"redloss_w_sCNN\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"].isin([v for (k,v) in irlomo_lookup_reverse.items() if \"small CNN\" in k])),\n",
    "}\n",
    "\n",
    "unif_df = df[conditions[\"uniform\"]]\n",
    "redloss_df = df[conditions[which_irlomo]]\n",
    "\n",
    "grouped = redloss_df.groupby([\"irreducible_loss_generator/f\"])\n",
    "speedups = {}\n",
    "for name, group in grouped:\n",
    "    speedup = compute_speedups(pd.concat((group, unif_df)), reference_epoch)\n",
    "    speedups[irlomo_lookup[name]] = speedup\n",
    "\n",
    "print(speedups)\n",
    "with open(os.path.join(save_dir_speedups, exp_name + \".pkl\"), \"wb\") as f:\n",
    "    pickle.dump(speedups, f)\n",
    "\n",
    "all_speedups[exp_name] = speedups\n",
    "\n",
    "\n",
    "#####------------------------------#####\n",
    "exp_name = \"CIFAR10_double_IrLoMo\"\n",
    "reference_epoch = cifar10_reference_epoch\n",
    "which_irlomo = cifar10_which_irlomo\n",
    "\n",
    "df = load_df(exp_name)\n",
    "\n",
    "irlomo_lookup_reverse = {\n",
    "    \"Resnet18, double IrLoMo\": \"/users/janner/goldiprox-hydra/logs/runs/2021-11-17/17-46-03/checkpoints/irred_losses_and_checks_double_irrlomo.pt\",\n",
    "    \"small CNN, double IrLoMo\": \"/users/janner/goldiprox-hydra/logs/runs/2021-11-17/17-52-13/checkpoints/irred_losses_and_checks_double_irrlomo.pt\",\n",
    "}\n",
    "\n",
    "irlomo_lookup = {v: k for (k,v) in irlomo_lookup_reverse.items()}\n",
    "\n",
    "conditions = {\n",
    "    \"uniform\": df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.uniform_selection\",\n",
    "    \"redloss_w_resnet18\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"].isin([v for (k,v) in irlomo_lookup_reverse.items() if \"Resnet18\" in k])), \n",
    "    \"redloss_w_sCNN\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"].isin([v for (k,v) in irlomo_lookup_reverse.items() if \"small CNN\" in k])),\n",
    "}\n",
    "\n",
    "unif_df = df[conditions[\"uniform\"]]\n",
    "redloss_df = df[conditions[which_irlomo]]\n",
    "\n",
    "grouped = redloss_df.groupby([\"irreducible_loss_generator/f\"])\n",
    "speedups = {}\n",
    "for name, group in grouped:\n",
    "    speedup = compute_speedups(pd.concat((group, unif_df)), reference_epoch)\n",
    "    speedups[irlomo_lookup[name]] = speedup\n",
    "\n",
    "print(speedups)\n",
    "with open(os.path.join(save_dir_speedups, exp_name + \".pkl\"), \"wb\") as f:\n",
    "    pickle.dump(speedups, f)\n",
    "\n",
    "all_speedups[exp_name] = speedups\n",
    "\n",
    "\n",
    "#####------------------------------#####\n",
    "exp_name = \"CIFAR10_small_CNN\"\n",
    "reference_epoch = cifar10_reference_epoch\n",
    "\n",
    "df = load_df(exp_name)\n",
    "\n",
    "speedups = {}\n",
    "speedup = compute_speedups(df, reference_epoch)\n",
    "speedups[\"small_cnn\"] = speedup\n",
    "\n",
    "print(speedups)\n",
    "with open(os.path.join(save_dir_speedups, exp_name + \".pkl\"), \"wb\") as f:\n",
    "    pickle.dump(speedups, f)\n",
    "\n",
    "all_speedups[exp_name] = speedups\n",
    "\n",
    "#####------------------------------#####\n",
    "exp_name = \"CIFAR10_default\"\n",
    "reference_epoch = cifar10_reference_epoch\n",
    "\n",
    "df = load_df(exp_name)\n",
    "\n",
    "speedups = {}\n",
    "speedup = compute_speedups(df, reference_epoch)\n",
    "speedups[\"default\"] = speedup\n",
    "\n",
    "print(speedups)\n",
    "with open(os.path.join(save_dir_speedups, exp_name + \".pkl\"), \"wb\") as f:\n",
    "    pickle.dump(speedups, f)\n",
    "\n",
    "all_speedups[exp_name] = speedups\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f92a8c4",
   "metadata": {},
   "source": [
    "## CINIC-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22bc3d8",
   "metadata": {},
   "source": [
    "### settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eded3061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch in uniform training in reference to which the speedups are computed\n",
    "cinic10_reference_epoch = 99\n",
    "\n",
    "# determines which irreducible loss model to use for computing the speedups.\n",
    "# Options are (\"redloss_w_resnet18\", \"redloss_w_sCNN\"). Does not influence\n",
    "# \"default\" or \"small_CNN\" experiments, of course (these are alwas Resnet18 and\n",
    "# small CNN, respectively)\n",
    "cinic10_which_irlomo = \"redloss_w_sCNN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb217e2a",
   "metadata": {},
   "source": [
    "### computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ede94e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no redloss runs, can't compute speedup\n",
      "no redloss runs, can't compute speedup\n",
      "no redloss runs, can't compute speedup\n",
      "no redloss runs, can't compute speedup\n",
      "no uniform runs, can't compute speedup\n",
      "no redloss runs, can't compute speedup\n",
      "no redloss runs, can't compute speedup\n",
      "no redloss runs, can't compute speedup\n",
      "no redloss runs, can't compute speedup\n",
      "{(0.0001, 160, 0.001): [nan], (0.0001, 160, 0.1): [nan], (0.0001, 640, 0.001): [1.8181818181818181], (0.0001, 640, 0.01): [nan], (0.0001, 640, 0.1): [1.8867924528301887], (0.0001, 960, 0.001): [1.6129032258064515], (0.0001, 960, 0.01): [1.639344262295082], (0.0001, 960, 0.1): [1.7241379310344827], (0.001, 160, 0.001): [2.380952380952381], (0.001, 160, 0.01): [2.380952380952381], (0.001, 160, 0.1): [4.545454545454546], (0.001, 640, 0.001): [nan], (0.001, 640, 0.01): [nan], (0.001, 640, 0.1): [2.272727272727273], (0.001, 960, 0.001): [1.5873015873015872], (0.001, 960, 0.01): [1.5873015873015872], (0.001, 960, 0.1): [2.2222222222222223], (0.01, 160, 0.001): [nan], (0.01, 160, 0.01): [nan], (0.01, 160, 0.1): [nan], (0.01, 640, 0.001): [1.8867924528301887], (0.01, 640, 0.01): [nan], (0.01, 640, 0.1): [2.7777777777777777], (0.01, 960, 0.001): [1.7241379310344827], (0.01, 960, 0.01): [1.5873015873015872], (0.01, 960, 0.1): [2.857142857142857]}\n",
      "{'src.models.modules.cifar_model_zoo.densenet.densenet121': [2.4390243902439024, 2.1739130434782608], 'src.models.modules.cifar_model_zoo.googlenet.googlenet': [2.7027027027027026, 2.857142857142857], 'src.models.modules.cifar_model_zoo.inception.inception_v3': [2.5, 2.7027027027027026], 'src.models.modules.cifar_model_zoo.mobilenetv2.mobilenet_v2': [1.9607843137254901, 1.9607843137254901], 'src.models.modules.cifar_model_zoo.resnet.resnet34': [2.127659574468085, 2.3255813953488373], 'src.models.modules.cifar_model_zoo.resnet.resnet50': [2.0833333333333335, 2.272727272727273], 'src.models.modules.cifar_model_zoo.vgg.vgg11_bn': [1.5873015873015872, 1.4492753623188406], 'src.models.modules.resnet_cifar.ResNet18': [2.380952380952381, 0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_298696/184083434.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unif_df[\"cummax_val_acc_epoch\"] = unif_df.cummax()[\"val_acc_epoch\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'small CNN, 1': [1.9230769230769231, 1.7543859649122806, 1.6129032258064515], 'small CNN, 0.25': [1.2195121951219512, 0.7299270072992701], 'small CNN, 0.5': [1.5151515151515151, 1.2658227848101267], 'small CNN, 0.75': [1.639344262295082, 1.5625], 'small CNN, 0.33': [1.1764705882352942, 1.2987012987012987]}\n",
      "{'small CNN, double IrLoMo': [1.1627906976744187, 1.0204081632653061]}\n",
      "{'small_cnn': [1.9230769230769231, 1.7543859649122806, 1.6129032258064515]}\n",
      "{'default': [1.7857142857142858, 1.8181818181818181]}\n"
     ]
    }
   ],
   "source": [
    "#####------------------------------#####\n",
    "exp_name = \"CINIC10_hypers\"\n",
    "reference_epoch = cinic10_reference_epoch\n",
    "which_irlomo = cinic10_which_irlomo\n",
    "\n",
    "df = load_df(exp_name)\n",
    "\n",
    "conditions = {\n",
    "    \"uniform\": df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.uniform_selection\",\n",
    "    \"redloss_w_resnet18\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"] == \"/data/stats-sgmcmc/magd5198/goldiprox-hydra/logs/runs/2021-10-01/13-18-09/checkpoints/irred_losses_and_checks_degraded_1.pt\"), \n",
    "    \"redloss_w_sCNN\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"] == \"/data/stats-sgmcmc/magd5198/goldiprox-hydra/logs/runs/2021-11-16/14-46-16/checkpoints/irred_losses_and_checks_degraded_1.pt\") \n",
    "}\n",
    "\n",
    "filtered = df[conditions[\"uniform\"] | conditions[\"redloss_w_sCNN\"]]\n",
    "grouped = filtered.groupby([\"optimizer/lr\", \"datamodule/batch_size\", \"optimizer/weight_decay\"])\n",
    "speedups = {}\n",
    "for name, group in grouped:\n",
    "    # print(group)\n",
    "    speedup = compute_speedups(group, reference_epoch)\n",
    "    speedups[name] = speedup\n",
    "\n",
    "print(speedups)\n",
    "with open(\"CIFAR10_hypers.pkl\", \"wb\") as f:\n",
    "    pickle.dump(speedups, f)\n",
    "\n",
    "\n",
    "all_speedups[exp_name] = speedups\n",
    "\n",
    "\n",
    "#####------------------------------#####\n",
    "exp_name = \"CINIC10_archs\"\n",
    "reference_epoch = cinic10_reference_epoch\n",
    "which_irlomo = cinic10_which_irlomo\n",
    "\n",
    "df = load_df(exp_name)\n",
    "\n",
    "conditions = {\n",
    "    \"uniform\": df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.uniform_selection\",\n",
    "    \"redloss_w_resnet18\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"] == \"/data/stats-sgmcmc/magd5198/goldiprox-hydra/logs/runs/2021-10-01/13-18-09/checkpoints/irred_losses_and_checks_degraded_1.pt\"), \n",
    "    \"redloss_w_sCNN\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"] == \"/data/stats-sgmcmc/magd5198/goldiprox-hydra/logs/runs/2021-11-16/14-46-16/checkpoints/irred_losses_and_checks_degraded_1.pt\") \n",
    "}\n",
    "\n",
    "filtered = df[conditions[\"uniform\"] | conditions[which_irlomo]]\n",
    "grouped = filtered.groupby([\"model/large_model/_target_\"])\n",
    "speedups = {}\n",
    "for name, group in grouped:\n",
    "    speedup = compute_speedups(group, reference_epoch)\n",
    "    speedups[name] = speedup\n",
    "\n",
    "print(speedups)\n",
    "with open(os.path.join(save_dir_speedups, exp_name + \".pkl\"), \"wb\") as f:\n",
    "    pickle.dump(speedups, f)\n",
    "\n",
    "all_speedups[exp_name] = speedups\n",
    "\n",
    "\n",
    "#####------------------------------#####\n",
    "exp_name = \"CINIC10_holdout_set\"\n",
    "reference_epoch = cinic10_reference_epoch\n",
    "which_irlomo = cinic10_which_irlomo\n",
    "\n",
    "df = load_df(exp_name)\n",
    "\n",
    "irlomo_lookup_reverse = {\n",
    "    \"Resnet18, 1\": \"/data/stats-sgmcmc/magd5198/goldiprox-hydra/logs/runs/2021-10-01/13-18-09/checkpoints/irred_losses_and_checks_degraded_1.pt\",\n",
    "    \"Resnet18, 0.75\": \"/data/stats-sgmcmc/magd5198/goldiprox-hydra/logs/runs/2021-11-18/16-28-33/checkpoints/irred_losses_and_checks.pt\",\n",
    "    \"Resnet18, 0.5\": \"/data/stats-sgmcmc/magd5198/goldiprox-hydra/logs/runs/2021-11-18/16-28-25/checkpoints/irred_losses_and_checks.pt\",\n",
    "    \"Resnet18, 0.33\": \"/data/stats-sgmcmc/magd5198/goldiprox-hydra/logs/runs/2021-11-18/16-28-13/checkpoints/irred_losses_and_checks.pt\",\n",
    "    \"Resnet18, 0.25\": \"/data/stats-sgmcmc/magd5198/goldiprox-hydra/logs/runs/2021-11-18/16-27-58/checkpoints/irred_losses_and_checks.pt\",\n",
    "    \"small CNN, 1\": \"/data/stats-sgmcmc/magd5198/goldiprox-hydra/logs/runs/2021-11-16/14-46-16/checkpoints/irred_losses_and_checks_degraded_1.pt\",\n",
    "    \"small CNN, 0.75\": \"/data/stats-sgmcmc/magd5198/goldiprox-hydra/logs/runs/2021-11-18/16-41-55/checkpoints/irred_losses_and_checks.pt\",\n",
    "    \"small CNN, 0.5\": \"/data/stats-sgmcmc/magd5198/goldiprox-hydra/logs/runs/2021-11-18/16-29-10/checkpoints/irred_losses_and_checks.pt\",\n",
    "    \"small CNN, 0.33\": \"/data/stats-sgmcmc/magd5198/goldiprox-hydra/logs/runs/2021-11-19/14-33-56/checkpoints/irred_losses_and_checks.pt\",\n",
    "    \"small CNN, 0.25\": \"/data/stats-sgmcmc/magd5198/goldiprox-hydra/logs/runs/2021-11-18/16-29-06/checkpoints/irred_losses_and_checks.pt\",\n",
    "}\n",
    "\n",
    "irlomo_lookup = {v: k for (k,v) in irlomo_lookup_reverse.items()}\n",
    "\n",
    "conditions = {\n",
    "    \"uniform\": df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.uniform_selection\",\n",
    "    \"redloss_w_resnet18\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"].isin([v for (k,v) in irlomo_lookup_reverse.items() if \"Resnet18\" in k])), \n",
    "    \"redloss_w_sCNN\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"].isin([v for (k,v) in irlomo_lookup_reverse.items() if \"small CNN\" in k])),\n",
    "}\n",
    "\n",
    "unif_df = df[conditions[\"uniform\"]]\n",
    "redloss_df = df[conditions[which_irlomo]]\n",
    "\n",
    "grouped = redloss_df.groupby([\"irreducible_loss_generator/f\"])\n",
    "speedups = {}\n",
    "for name, group in grouped:\n",
    "    speedup = compute_speedups(pd.concat((group, unif_df)), reference_epoch)\n",
    "    speedups[irlomo_lookup[name]] = speedup\n",
    "\n",
    "print(speedups)\n",
    "with open(os.path.join(save_dir_speedups, exp_name + \".pkl\"), \"wb\") as f:\n",
    "    pickle.dump(speedups, f)\n",
    "\n",
    "all_speedups[exp_name] = speedups\n",
    "\n",
    "\n",
    "#####------------------------------#####\n",
    "exp_name = \"CINIC10_double_IrLoMo\"\n",
    "reference_epoch = cinic10_reference_epoch\n",
    "which_irlomo = cinic10_which_irlomo\n",
    "\n",
    "df = load_df(exp_name)\n",
    "\n",
    "irlomo_lookup_reverse = {\n",
    "    \"Resnet18, double IrLoMo\": \"/data/stats-sgmcmc/magd5198/goldiprox-hydra/logs/runs/2021-11-20/17-33-49/checkpoints/irred_losses_and_checks_double_irrlomo.pt\",\n",
    "    \"small CNN, double IrLoMo\": \"/data/stats-sgmcmc/magd5198/goldiprox-hydra/logs/runs/2021-11-20/17-33-32/checkpoints/irred_losses_and_checks_double_irrlomo.pt\",\n",
    "}\n",
    "irlomo_lookup = {v: k for (k,v) in irlomo_lookup_reverse.items()}\n",
    "\n",
    "conditions = {\n",
    "    \"uniform\": df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.uniform_selection\",\n",
    "    \"redloss_w_resnet18\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"].isin([v for (k,v) in irlomo_lookup_reverse.items() if \"Resnet18\" in k])), \n",
    "    \"redloss_w_sCNN\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"].isin([v for (k,v) in irlomo_lookup_reverse.items() if \"small CNN\" in k])),\n",
    "}\n",
    "\n",
    "unif_df = df[conditions[\"uniform\"]]\n",
    "redloss_df = df[conditions[which_irlomo]]\n",
    "\n",
    "grouped = redloss_df.groupby([\"irreducible_loss_generator/f\"])\n",
    "speedups = {}\n",
    "for name, group in grouped:\n",
    "    speedup = compute_speedups(pd.concat((group, unif_df)), reference_epoch)\n",
    "    speedups[irlomo_lookup[name]] = speedup\n",
    "\n",
    "print(speedups)\n",
    "with open(os.path.join(save_dir_speedups, exp_name + \".pkl\"), \"wb\") as f:\n",
    "    pickle.dump(speedups, f)\n",
    "\n",
    "all_speedups[exp_name] = speedups\n",
    "\n",
    "\n",
    "#####------------------------------#####\n",
    "exp_name = \"CINIC10_small_CNN\"\n",
    "reference_epoch = cinic10_reference_epoch\n",
    "\n",
    "df = load_df(exp_name)\n",
    "\n",
    "irlomo_lookup_reverse = {\n",
    "    \"small CNN, 1\": \"/data/stats-sgmcmc/magd5198/goldiprox-hydra/logs/runs/2021-11-16/14-46-16/checkpoints/irred_losses_and_checks_degraded_1.pt\",\n",
    "}\n",
    "\n",
    "irlomo_lookup = {v: k for (k,v) in irlomo_lookup_reverse.items()}\n",
    "\n",
    "conditions = {\n",
    "    \"uniform\": df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.uniform_selection\",\n",
    "    \"redloss_w_sCNN\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"].isin([v for (k,v) in irlomo_lookup_reverse.items() if \"small CNN\" in k])),\n",
    "}\n",
    "\n",
    "unif_df = df[conditions[\"uniform\"]]\n",
    "redloss_df = df[conditions[\"redloss_w_sCNN\"]]\n",
    "\n",
    "speedups = {}\n",
    "speedup = compute_speedups(pd.concat((redloss_df, unif_df)), reference_epoch)\n",
    "speedups[\"small_cnn\"] = speedup\n",
    "\n",
    "print(speedups)\n",
    "with open(os.path.join(save_dir_speedups, exp_name + \".pkl\"), \"wb\") as f:\n",
    "    pickle.dump(speedups, f)\n",
    "\n",
    "all_speedups[exp_name] = speedups\n",
    "\n",
    "\n",
    "#####------------------------------#####\n",
    "exp_name = \"CINIC10_default\"\n",
    "reference_epoch = cinic10_reference_epoch\n",
    "\n",
    "df = load_df(exp_name)\n",
    "\n",
    "irlomo_lookup_reverse = {\n",
    "    \"Resnet18, 1\": \"/data/stats-sgmcmc/magd5198/goldiprox-hydra/logs/runs/2021-10-01/13-18-09/checkpoints/irred_losses_and_checks_degraded_1.pt\",\n",
    "}\n",
    "\n",
    "irlomo_lookup = {v: k for (k,v) in irlomo_lookup_reverse.items()}\n",
    "\n",
    "conditions = {\n",
    "    \"uniform\": df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.uniform_selection\",\n",
    "    \"redloss_w_resnet18\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"].isin([v for (k,v) in irlomo_lookup_reverse.items() if \"Resnet18\" in k])), \n",
    "}\n",
    "\n",
    "unif_df = df[conditions[\"uniform\"]]\n",
    "redloss_df = df[conditions[\"redloss_w_resnet18\"]]\n",
    "\n",
    "speedups = {}\n",
    "speedup = compute_speedups(pd.concat((redloss_df, unif_df)), reference_epoch)\n",
    "speedups[\"default\"] = speedup\n",
    "\n",
    "print(speedups)\n",
    "with open(os.path.join(save_dir_speedups, exp_name + \".pkl\"), \"wb\") as f:\n",
    "    pickle.dump(speedups, f)\n",
    "\n",
    "all_speedups[exp_name] = speedups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e350a26",
   "metadata": {},
   "source": [
    "## CIFAR-100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f47ecc",
   "metadata": {},
   "source": [
    "### settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edb69e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch in uniform training in reference to which the speedups are computed\n",
    "cifar100_reference_epoch = 99\n",
    "\n",
    "# determines which irreducible loss model to use for computing the speedups.\n",
    "# Options are (\"redloss_w_resnet18\", \"redloss_w_sCNN\"). Does not influence\n",
    "# \"default\" or \"small_CNN\" experiments, of course (these are alwas Resnet18 and\n",
    "# small CNN, respectively)\n",
    "cifar100_which_irlomo = \"redloss_w_sCNN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de145440",
   "metadata": {},
   "source": [
    "### computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d661e7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_298696/184083434.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unif_df[\"cummax_val_acc_epoch\"] = unif_df.cummax()[\"val_acc_epoch\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no redloss runs, can't compute speedup\n",
      "{(0.0001, 160, 0.001): [4.95], (0.0001, 160, 0.01): [4.5], (0.0001, 160, 0.1): [4.5], (0.0001, 320, 0.001): [4.125], (0.0001, 320, 0.01): [4.304347826086956], (0.0001, 320, 0.1): [3.6666666666666665], (0.0001, 960, 0.001): [0], (0.0001, 960, 0.01): [1.706896551724138], (0.0001, 960, 0.1): [1.736842105263158], (0.001, 160, 0.001): [3.5357142857142856], (0.001, 160, 0.01): [3.6666666666666665], (0.001, 160, 0.1): [3.8076923076923075], (0.001, 320, 0.001): [3.413793103448276], (0.001, 320, 0.1): [3.6666666666666665], (0.001, 960, 0.001): [nan], (0.001, 960, 0.01): [3.3], (0.001, 960, 0.1): [2.6052631578947367], (0.01, 160, 0.001): [2.5384615384615383], (0.01, 160, 0.01): [2.675675675675676], (0.01, 160, 0.1): [6.1875], (0.01, 320, 0.001): [2.6052631578947367], (0.01, 320, 0.01): [2.357142857142857], (0.01, 320, 0.1): [3.5357142857142856], (0.01, 960, 0.001): [1.98], (0.01, 960, 0.01): [1.4558823529411764], (0.01, 960, 0.1): [2.357142857142857]}\n",
      "{'src.models.modules.cifar_model_zoo.densenet.densenet121': [2.302325581395349, 2.020408163265306], 'src.models.modules.cifar_model_zoo.googlenet.googlenet': [2.0625, 2.152173913043478], 'src.models.modules.cifar_model_zoo.inception.inception_v3': [2.25, 1.98], 'src.models.modules.cifar_model_zoo.mobilenetv2.mobilenet_v2': [1.98, 2.357142857142857], 'src.models.modules.cifar_model_zoo.resnet.resnet18': [3.8076923076923075, 4.125], 'src.models.modules.cifar_model_zoo.resnet.resnet34': [2.5384615384615383, 2.75], 'src.models.modules.cifar_model_zoo.resnet.resnet50': [3.0, 2.152173913043478], 'src.models.modules.cifar_model_zoo.vgg.vgg11_bn': [2.302325581395349, 1.9411764705882353]}\n",
      "{'small CNN, 0.25': [1.2375, 1.523076923076923], 'small CNN, 0.33': [2.5384615384615383, 2.25], 'small CNN, 0.5': [2.675675675675676, 2.0625], 'small CNN, 0.75': [3.413793103448276, 2.25]}\n",
      "{'small CNN, double IrLoMo': [1.1785714285714286, 2.302325581395349, 1.9411764705882353]}\n",
      "{'small_cnn': [4.125, 3.413793103448276]}\n",
      "{'default': [4.125, 3.413793103448276]}\n"
     ]
    }
   ],
   "source": [
    "#####------------------------------#####\n",
    "exp_name = \"CIFAR100_hypers\"\n",
    "reference_epoch = cifar100_reference_epoch\n",
    "which_irlomo = cifar100_which_irlomo\n",
    "\n",
    "df = load_df(exp_name)\n",
    "\n",
    "conditions = {\n",
    "    \"uniform\": df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.uniform_selection\",\n",
    "    \"redloss_w_resnet18\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"] == \"/home/yarin.oxford/mo/goldiprox-hydra/logs/multiruns/2021-11-20_09-02-23/0/checkpoints/irred_losses_and_checks.pt\"), \n",
    "    \"redloss_w_sCNN\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"] == \"/home/yarin.oxford/mo/goldiprox-hydra/logs/multiruns/2021-11-20_09-02-56/0/checkpoints/irred_losses_and_checks.pt\") \n",
    "}\n",
    "\n",
    "filtered = df[conditions[\"uniform\"] | conditions[which_irlomo]]\n",
    "grouped = filtered.groupby([\"optimizer/lr\", \"datamodule/batch_size\", \"optimizer/weight_decay\"])\n",
    "speedups = {}\n",
    "for name, group in grouped:\n",
    "    # print(group)\n",
    "    speedup = compute_speedups(group, reference_epoch)\n",
    "    speedups[name] = speedup\n",
    "\n",
    "print(speedups)\n",
    "with open(os.path.join(save_dir_speedups, exp_name + \".pkl\"), \"wb\") as f:\n",
    "    pickle.dump(speedups, f)\n",
    "\n",
    "all_speedups[exp_name] = speedups\n",
    "\n",
    "\n",
    "#####------------------------------#####\n",
    "exp_name = \"CIFAR100_archs\"\n",
    "reference_epoch = cifar100_reference_epoch\n",
    "which_irlomo = cifar100_which_irlomo\n",
    "\n",
    "df = load_df(exp_name)\n",
    "\n",
    "conditions = {\n",
    "    \"uniform\": df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.uniform_selection\",\n",
    "    \"redloss_w_sCNN\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") # no resnet IrLoMo runs available here\n",
    "}\n",
    "\n",
    "filtered = df[conditions[\"uniform\"] | conditions[which_irlomo]]\n",
    "grouped = filtered.groupby([\"model/large_model/_target_\"])\n",
    "speedups = {}\n",
    "for name, group in grouped:\n",
    "    speedup = compute_speedups(group, reference_epoch)\n",
    "    speedups[name] = speedup\n",
    "\n",
    "print(speedups)\n",
    "with open(os.path.join(save_dir_speedups, exp_name + \".pkl\"), \"wb\") as f:\n",
    "    pickle.dump(speedups, f)\n",
    "\n",
    "all_speedups[exp_name] = speedups\n",
    "\n",
    "\n",
    "#####------------------------------#####\n",
    "exp_name = \"CIFAR100_holdout_set\"\n",
    "reference_epoch = cifar100_reference_epoch\n",
    "which_irlomo = cifar100_which_irlomo\n",
    "\n",
    "df = load_df(exp_name)\n",
    "\n",
    "# automatic generation of irlomo_loookup_tabel from tags\n",
    "irlomo_lookup_reverse_temp = {\n",
    "    \"Resnet18, 0.75\": (\"0.75\", \"resnet\"),\n",
    "    \"Resnet18, 0.5\": (\"0.5\", \"resnet\"),\n",
    "    \"Resnet18, 0.33\": (\"0.33\", \"resnet\"),\n",
    "    \"Resnet18, 0.25\": (\"0.25\", \"resnet\"),\n",
    "    \"small CNN, 0.75\": (\"0.75\", \"small_cnn\"),\n",
    "    \"small CNN, 0.5\": (\"0.5\", \"small_cnn\"),\n",
    "    \"small CNN, 0.33\": (\"0.33\", \"small_cnn\"),\n",
    "    \"small CNN, 0.25\": (\"0.25\", \"small_cnn\"),\n",
    "}\n",
    "\n",
    "irlomo_lookup_reverse = {}\n",
    "\n",
    "for k,v in irlomo_lookup_reverse_temp.items():\n",
    "    df_subset = df[df[\"logger/wandb/tags\"].apply(lambda x: x == list(v))]\n",
    "    assert len(df_subset[\"irreducible_loss_generator/checkpoint_path\"].unique()) == 1\n",
    "    irlomo_lookup_reverse[k] = df_subset[\"irreducible_loss_generator/checkpoint_path\"].iloc[0]\n",
    "\n",
    "irlomo_lookup = {v: k for (k,v) in irlomo_lookup_reverse.items()}\n",
    "\n",
    "conditions = {\n",
    "    \"uniform\": df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.uniform_selection\",\n",
    "    \"redloss_w_resnet18\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/checkpoint_path\"].isin([v for (k,v) in irlomo_lookup_reverse.items() if \"Resnet18\" in k])), \n",
    "    \"redloss_w_sCNN\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/checkpoint_path\"].isin([v for (k,v) in irlomo_lookup_reverse.items() if \"small CNN\" in k])),\n",
    "}\n",
    "\n",
    "unif_df = df[conditions[\"uniform\"]]\n",
    "redloss_df = df[conditions[which_irlomo]]\n",
    "\n",
    "grouped = redloss_df.groupby([\"irreducible_loss_generator/checkpoint_path\"])\n",
    "speedups = {}\n",
    "for name, group in grouped:\n",
    "    speedup = compute_speedups(pd.concat((group, unif_df)), reference_epoch)\n",
    "    speedups[irlomo_lookup[name]] = speedup\n",
    "\n",
    "print(speedups)\n",
    "with open(os.path.join(save_dir_speedups, exp_name + \".pkl\"), \"wb\") as f:\n",
    "    pickle.dump(speedups, f)\n",
    "\n",
    "all_speedups[exp_name] = speedups\n",
    "\n",
    "\n",
    "#####------------------------------#####\n",
    "exp_name = \"CIFAR100_double_IrLoMo\"\n",
    "reference_epoch = cifar100_reference_epoch\n",
    "which_irlomo = cifar100_which_irlomo\n",
    "\n",
    "df = load_df(exp_name)\n",
    "\n",
    "# automatic generation of irlomo_loookup_tabel from tags\n",
    "irlomo_lookup_reverse_temp = {\n",
    "    \"Resnet18, double IrLoMo\": \"resnet\",\n",
    "    \"small CNN, double IrLoMo\": \"small_cnn\",\n",
    "}\n",
    "\n",
    "irlomo_lookup_reverse = {}\n",
    "\n",
    "for k,v in irlomo_lookup_reverse_temp.items():\n",
    "    df_subset = df[df[\"logger/wandb/tags\"].apply(lambda x: v in x)]\n",
    "    assert len(df_subset[\"irreducible_loss_generator/f\"].unique()) == 1\n",
    "    irlomo_lookup_reverse[k] = df_subset[\"irreducible_loss_generator/f\"].iloc[0]\n",
    "\n",
    "irlomo_lookup = {v: k for (k,v) in irlomo_lookup_reverse.items()}\n",
    "\n",
    "conditions = {\n",
    "    \"uniform\": df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.uniform_selection\",\n",
    "    \"redloss_w_resnet18\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"].isin([v for (k,v) in irlomo_lookup_reverse.items() if \"Resnet18\" in k])), \n",
    "    \"redloss_w_sCNN\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/f\"].isin([v for (k,v) in irlomo_lookup_reverse.items() if \"small CNN\" in k])),\n",
    "}\n",
    "\n",
    "unif_df = df[conditions[\"uniform\"]]\n",
    "redloss_df = df[conditions[which_irlomo]]\n",
    "\n",
    "grouped = redloss_df.groupby([\"irreducible_loss_generator/f\"])\n",
    "speedups = {}\n",
    "for name, group in grouped:\n",
    "    speedup = compute_speedups(pd.concat((group, unif_df)), reference_epoch)\n",
    "    speedups[irlomo_lookup[name]] = speedup\n",
    "\n",
    "print(speedups)\n",
    "with open(os.path.join(save_dir_speedups, exp_name + \".pkl\"), \"wb\") as f:\n",
    "    pickle.dump(speedups, f)\n",
    "\n",
    "all_speedups[exp_name] = speedups\n",
    "\n",
    "\n",
    "#####------------------------------#####\n",
    "exp_name = \"CIFAR100_small_CNN\"\n",
    "reference_epoch = cifar100_reference_epoch\n",
    "\n",
    "df = load_df(exp_name)\n",
    "\n",
    "# automatic generation of irlomo_loookup_tabel from tags\n",
    "irlomo_lookup_reverse_temp = {\n",
    "    \"small CNN, 1\": (\"one\", \"smallcnn\"),\n",
    "}\n",
    "\n",
    "irlomo_lookup_reverse = {}\n",
    "\n",
    "for k,v in irlomo_lookup_reverse_temp.items():\n",
    "    df_subset = df[df[\"logger/wandb/tags\"].apply(lambda x: x == list(v))]\n",
    "    assert len(df_subset[\"irreducible_loss_generator/checkpoint_path\"].unique()) == 1\n",
    "    irlomo_lookup_reverse[k] = df_subset[\"irreducible_loss_generator/checkpoint_path\"].iloc[0]\n",
    "\n",
    "irlomo_lookup = {v: k for (k,v) in irlomo_lookup_reverse.items()}\n",
    "\n",
    "conditions = {\n",
    "    \"uniform\": df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.uniform_selection\",\n",
    "    \"redloss_w_sCNN\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/checkpoint_path\"].isin([v for (k,v) in irlomo_lookup_reverse.items() if \"small CNN\" in k])),\n",
    "}\n",
    "\n",
    "unif_df = df[conditions[\"uniform\"]]\n",
    "redloss_df = df[conditions[\"redloss_w_sCNN\"]]\n",
    "\n",
    "speedups = {}\n",
    "speedup = compute_speedups(pd.concat((redloss_df, unif_df)), reference_epoch)\n",
    "speedups[\"small_cnn\"] = speedup\n",
    "\n",
    "print(speedups)\n",
    "with open(os.path.join(save_dir_speedups, exp_name + \".pkl\"), \"wb\") as f:\n",
    "    pickle.dump(speedups, f)\n",
    "\n",
    "all_speedups[exp_name] = speedups\n",
    "\n",
    "\n",
    "#####------------------------------#####\n",
    "exp_name = \"CIFAR100_default\"\n",
    "reference_epoch = cifar100_reference_epoch\n",
    "\n",
    "df = load_df(exp_name)\n",
    "\n",
    "# automatic generation of irlomo_loookup_tabel from tags\n",
    "irlomo_lookup_reverse_temp = {\n",
    "    \"Resnet18, 1\": (\"one\", \"resnet\"),\n",
    "}\n",
    "\n",
    "irlomo_lookup_reverse = {}\n",
    "\n",
    "for k,v in irlomo_lookup_reverse_temp.items():\n",
    "    df_subset = df[df[\"logger/wandb/tags\"].apply(lambda x: x == list(v))]\n",
    "    assert len(df_subset[\"irreducible_loss_generator/checkpoint_path\"].unique()) == 1\n",
    "    irlomo_lookup_reverse[k] = df_subset[\"irreducible_loss_generator/checkpoint_path\"].iloc[0]\n",
    "\n",
    "irlomo_lookup = {v: k for (k,v) in irlomo_lookup_reverse.items()}\n",
    "\n",
    "conditions = {\n",
    "    \"uniform\": df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.uniform_selection\",\n",
    "    \"redloss_w_resnet18\": (df[\"selection_method/_target_\"] == \"src.curricula.selection_methods.reducible_loss_selection\") & (df[\"irreducible_loss_generator/checkpoint_path\"].isin([v for (k,v) in irlomo_lookup_reverse.items() if \"Resnet18\" in k])), \n",
    "}\n",
    "\n",
    "unif_df = df[conditions[\"uniform\"]]\n",
    "redloss_df = df[conditions[\"redloss_w_resnet18\"]]\n",
    "\n",
    "speedups = {}\n",
    "speedup = compute_speedups(pd.concat((redloss_df, unif_df)), reference_epoch)\n",
    "speedups[\"default\"] = speedup\n",
    "\n",
    "print(speedups)\n",
    "with open(os.path.join(save_dir_speedups, exp_name + \".pkl\"), \"wb\") as f:\n",
    "    pickle.dump(speedups, f)\n",
    "\n",
    "all_speedups[exp_name] = speedups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d20b15d",
   "metadata": {},
   "source": [
    "# show all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "584efc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10_hypers\n",
      "\n",
      "{(0.0001, 160, 0.001): [2.127659574468085], (0.0001, 160, 0.01): [2.0833333333333335], (0.0001, 160, 0.1): [2.127659574468085], (0.0001, 320, 0.001): [2.0833333333333335], (0.0001, 320, 0.01): [2.0], (0.0001, 320, 0.1): [2.0833333333333335], (0.0001, 960, 0.001): [1.5384615384615385], (0.0001, 960, 0.01): [1.5625], (0.0001, 960, 0.1): [1.639344262295082], (0.001, 160, 0.001): [2.857142857142857], (0.001, 160, 0.01): [2.857142857142857], (0.001, 160, 0.1): [3.4482758620689653], (0.001, 320, 0.001): [2.5641025641025643], (0.001, 320, 0.01): [2.857142857142857], (0.001, 320, 0.1): [3.225806451612903], (0.001, 960, 0.001): [1.7857142857142858], (0.001, 960, 0.01): [1.639344262295082], (0.001, 960, 0.1): [2.0], (0.01, 160, 0.001): [1.6129032258064515], (0.01, 160, 0.01): [2.0], (0.01, 160, 0.1): [2.3255813953488373], (0.01, 320, 0.001): [1.492537313432836], (0.01, 320, 0.01): [1.8181818181818181], (0.01, 320, 0.1): [2.0833333333333335], (0.01, 960, 0.001): [0.970873786407767], (0.01, 960, 0.01): [0.9900990099009901], (0.01, 960, 0.1): [1.1764705882352942]}\n",
      "\n",
      "\n",
      "\n",
      "CIFAR10_archs\n",
      "\n",
      "{'src.models.modules.cifar_model_zoo.densenet.densenet121': [2.4390243902439024, 2.0833333333333335], 'src.models.modules.cifar_model_zoo.googlenet.googlenet': [2.4390243902439024, 2.7027027027027026], 'src.models.modules.cifar_model_zoo.inception.inception_v3': [2.2222222222222223, 2.0], 'src.models.modules.cifar_model_zoo.mobilenetv2.mobilenet_v2': [2.127659574468085, 2.0], 'src.models.modules.cifar_model_zoo.resnet.resnet18': [2.5641025641025643, 2.7777777777777777], 'src.models.modules.cifar_model_zoo.resnet.resnet34': [2.380952380952381, 2.3255813953488373], 'src.models.modules.cifar_model_zoo.resnet.resnet50': [2.127659574468085, 1.7857142857142858], 'src.models.modules.cifar_model_zoo.vgg.vgg11_bn': [0.8547008547008547, 0]}\n",
      "\n",
      "\n",
      "\n",
      "CIFAR10_holdout_set\n",
      "\n",
      "{'small CNN, 0.5': [2.7027027027027026, 2.7027027027027026], 'small CNN, 0.75': [3.0303030303030303, 2.857142857142857], 'small CNN, 0.25': [1.7543859649122806, 1.8867924528301887], 'small CNN, 0.33': [2.380952380952381, 2.272727272727273]}\n",
      "\n",
      "\n",
      "\n",
      "CIFAR10_double_IrLoMo\n",
      "\n",
      "{'small CNN, double IrLoMo': [2.2222222222222223, 2.5]}\n",
      "\n",
      "\n",
      "\n",
      "CIFAR10_small_CNN\n",
      "\n",
      "{'small_cnn': [2.7777777777777777, 2.857142857142857, 2.7027027027027026]}\n",
      "\n",
      "\n",
      "\n",
      "CIFAR10_default\n",
      "\n",
      "{'default': [2.3255813953488373, 2.4390243902439024, 2.272727272727273]}\n",
      "\n",
      "\n",
      "\n",
      "CINIC10_hypers\n",
      "\n",
      "{(0.0001, 160, 0.001): [nan], (0.0001, 160, 0.1): [nan], (0.0001, 640, 0.001): [1.8181818181818181], (0.0001, 640, 0.01): [nan], (0.0001, 640, 0.1): [1.8867924528301887], (0.0001, 960, 0.001): [1.6129032258064515], (0.0001, 960, 0.01): [1.639344262295082], (0.0001, 960, 0.1): [1.7241379310344827], (0.001, 160, 0.001): [2.380952380952381], (0.001, 160, 0.01): [2.380952380952381], (0.001, 160, 0.1): [4.545454545454546], (0.001, 640, 0.001): [nan], (0.001, 640, 0.01): [nan], (0.001, 640, 0.1): [2.272727272727273], (0.001, 960, 0.001): [1.5873015873015872], (0.001, 960, 0.01): [1.5873015873015872], (0.001, 960, 0.1): [2.2222222222222223], (0.01, 160, 0.001): [nan], (0.01, 160, 0.01): [nan], (0.01, 160, 0.1): [nan], (0.01, 640, 0.001): [1.8867924528301887], (0.01, 640, 0.01): [nan], (0.01, 640, 0.1): [2.7777777777777777], (0.01, 960, 0.001): [1.7241379310344827], (0.01, 960, 0.01): [1.5873015873015872], (0.01, 960, 0.1): [2.857142857142857]}\n",
      "\n",
      "\n",
      "\n",
      "CINIC10_archs\n",
      "\n",
      "{'src.models.modules.cifar_model_zoo.densenet.densenet121': [2.4390243902439024, 2.1739130434782608], 'src.models.modules.cifar_model_zoo.googlenet.googlenet': [2.7027027027027026, 2.857142857142857], 'src.models.modules.cifar_model_zoo.inception.inception_v3': [2.5, 2.7027027027027026], 'src.models.modules.cifar_model_zoo.mobilenetv2.mobilenet_v2': [1.9607843137254901, 1.9607843137254901], 'src.models.modules.cifar_model_zoo.resnet.resnet34': [2.127659574468085, 2.3255813953488373], 'src.models.modules.cifar_model_zoo.resnet.resnet50': [2.0833333333333335, 2.272727272727273], 'src.models.modules.cifar_model_zoo.vgg.vgg11_bn': [1.5873015873015872, 1.4492753623188406], 'src.models.modules.resnet_cifar.ResNet18': [2.380952380952381, 0]}\n",
      "\n",
      "\n",
      "\n",
      "CINIC10_holdout_set\n",
      "\n",
      "{'small CNN, 1': [1.9230769230769231, 1.7543859649122806, 1.6129032258064515], 'small CNN, 0.25': [1.2195121951219512, 0.7299270072992701], 'small CNN, 0.5': [1.5151515151515151, 1.2658227848101267], 'small CNN, 0.75': [1.639344262295082, 1.5625], 'small CNN, 0.33': [1.1764705882352942, 1.2987012987012987]}\n",
      "\n",
      "\n",
      "\n",
      "CINIC10_double_IrLoMo\n",
      "\n",
      "{'small CNN, double IrLoMo': [1.1627906976744187, 1.0204081632653061]}\n",
      "\n",
      "\n",
      "\n",
      "CINIC10_small_CNN\n",
      "\n",
      "{'small_cnn': [1.9230769230769231, 1.7543859649122806, 1.6129032258064515]}\n",
      "\n",
      "\n",
      "\n",
      "CINIC10_default\n",
      "\n",
      "{'default': [1.7857142857142858, 1.8181818181818181]}\n",
      "\n",
      "\n",
      "\n",
      "CIFAR100_hypers\n",
      "\n",
      "{(0.0001, 160, 0.001): [4.95], (0.0001, 160, 0.01): [4.5], (0.0001, 160, 0.1): [4.5], (0.0001, 320, 0.001): [4.125], (0.0001, 320, 0.01): [4.304347826086956], (0.0001, 320, 0.1): [3.6666666666666665], (0.0001, 960, 0.001): [0], (0.0001, 960, 0.01): [1.706896551724138], (0.0001, 960, 0.1): [1.736842105263158], (0.001, 160, 0.001): [3.5357142857142856], (0.001, 160, 0.01): [3.6666666666666665], (0.001, 160, 0.1): [3.8076923076923075], (0.001, 320, 0.001): [3.413793103448276], (0.001, 320, 0.1): [3.6666666666666665], (0.001, 960, 0.001): [nan], (0.001, 960, 0.01): [3.3], (0.001, 960, 0.1): [2.6052631578947367], (0.01, 160, 0.001): [2.5384615384615383], (0.01, 160, 0.01): [2.675675675675676], (0.01, 160, 0.1): [6.1875], (0.01, 320, 0.001): [2.6052631578947367], (0.01, 320, 0.01): [2.357142857142857], (0.01, 320, 0.1): [3.5357142857142856], (0.01, 960, 0.001): [1.98], (0.01, 960, 0.01): [1.4558823529411764], (0.01, 960, 0.1): [2.357142857142857]}\n",
      "\n",
      "\n",
      "\n",
      "CIFAR100_archs\n",
      "\n",
      "{'src.models.modules.cifar_model_zoo.densenet.densenet121': [2.302325581395349, 2.020408163265306], 'src.models.modules.cifar_model_zoo.googlenet.googlenet': [2.0625, 2.152173913043478], 'src.models.modules.cifar_model_zoo.inception.inception_v3': [2.25, 1.98], 'src.models.modules.cifar_model_zoo.mobilenetv2.mobilenet_v2': [1.98, 2.357142857142857], 'src.models.modules.cifar_model_zoo.resnet.resnet18': [3.8076923076923075, 4.125], 'src.models.modules.cifar_model_zoo.resnet.resnet34': [2.5384615384615383, 2.75], 'src.models.modules.cifar_model_zoo.resnet.resnet50': [3.0, 2.152173913043478], 'src.models.modules.cifar_model_zoo.vgg.vgg11_bn': [2.302325581395349, 1.9411764705882353]}\n",
      "\n",
      "\n",
      "\n",
      "CIFAR100_holdout_set\n",
      "\n",
      "{'small CNN, 0.25': [1.2375, 1.523076923076923], 'small CNN, 0.33': [2.5384615384615383, 2.25], 'small CNN, 0.5': [2.675675675675676, 2.0625], 'small CNN, 0.75': [3.413793103448276, 2.25]}\n",
      "\n",
      "\n",
      "\n",
      "CIFAR100_double_IrLoMo\n",
      "\n",
      "{'small CNN, double IrLoMo': [1.1785714285714286, 2.302325581395349, 1.9411764705882353]}\n",
      "\n",
      "\n",
      "\n",
      "CIFAR100_small_CNN\n",
      "\n",
      "{'small_cnn': [4.125, 3.413793103448276]}\n",
      "\n",
      "\n",
      "\n",
      "CIFAR100_default\n",
      "\n",
      "{'default': [4.125, 3.413793103448276]}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k,v in all_speedups.items():\n",
    "    print(k + \"\\n\")\n",
    "    print(v)\n",
    "    print(\"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c8f701",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e8320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_speedups(exp_name, path=\"\"):\n",
    "    try:\n",
    "        speedups = all_speedups[exp_name] #yes, yes, I probably shouldn't do it like this. Just don't call anything dfs, OK :-)\n",
    "    except:\n",
    "        with open(path + exp_name + \".pkl\", \"rb\") as f:\n",
    "            speedups = pickle.load(f)\n",
    "    return speedups"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfd0ac69e947a4c3f41d3e96b87afe4576cfb36da8142bbbf50a4fd2ea8d9d14"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
