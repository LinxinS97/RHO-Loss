{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223fbcee-c1be-4bf7-8a77-2aef9d343df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "\n",
    "import functools\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams[\"font.family\"] = \"Times\"\n",
    "plt.rcParams[\"font.weight\"] = \"light\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dcb8b3-429c-4a50-b71b-ea1999be8c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish and plot colorblind color pallete\n",
    "colors = sns.color_palette('colorblind')\n",
    "sns.palplot(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ca376f-6688-4114-b7c9-f59a2afec955",
   "metadata": {},
   "source": [
    "# Weights and Biases API Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c983e23c-9903-4d1d-9746-c2341db34959",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api(timeout=30)\n",
    "\n",
    "@functools.lru_cache(maxsize=2048)\n",
    "def extract_run_df(run, keys):\n",
    "    keys_to_extract = keys\n",
    "    set_keys_to_extract = set(keys)\n",
    "    \n",
    "    extracted_information = []\n",
    "    for row in run.scan_history(list(keys)):\n",
    "        extracted_row = {}\n",
    "        if len(set(row.keys()).intersection(set_keys_to_extract)) > 1:\n",
    "            for key in keys_to_extract:\n",
    "                if key in row.keys():\n",
    "                    extracted_row[key] = row[key]\n",
    "\n",
    "            extracted_information.append(extracted_row)\n",
    "    \n",
    "    run_df = pd.DataFrame(extracted_information)\n",
    "#     run_df = run_df.rename({\"_step\": \"step\"}, axis=1)\n",
    "    \n",
    "    return run_df\n",
    "\n",
    "def filter_runs_by_tag(tag, all_runs):\n",
    "    filtered_runs = [run for run in all_runs if tag in run.tags]\n",
    "    return filtered_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f416b-1646-4dde-ab6c-be16526fbac9",
   "metadata": {},
   "source": [
    "# More Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231b5982-7210-46ae-b69b-6e15f7c372f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cinic10_name_to_select_method_dict = {\n",
    "    \"uniform\": \"Uniform Sampling\", \n",
    "    \"_reducible_loss\": \"Reducible Loss (Ours)\",\n",
    "    \"importance_sampling\": \"Gradient Norm IS\",\n",
    "    \"_irreducible_loss\": \"Irreducible Loss\", \n",
    "    \"_gradnorm_ub\": \"Gradient Norm\", \n",
    "    \"_loss\": \"Loss\",\n",
    "}\n",
    "\n",
    "selection_methods = [\"Reducible Loss (Ours)\", \"Uniform Sampling\", \n",
    "                     \"Irreducible Loss\", \"Gradient Norm\", \"Loss\", \"SVP\",  \"Gradient Norm IS\"]\n",
    "\n",
    "selection_methods_color_dict = {\n",
    "    \"Reducible Loss (Ours)\": colors[0], \n",
    "    \"Reducible Loss (Ours)\\nSmall Irrloss Model\": colors[0], \n",
    "    \"Uniform Sampling\": colors[1], \n",
    "    \"Irreducible Loss\": colors[2],\n",
    "    \"Irreducible Loss\\nSmall Irrloss Model\": colors[2],\n",
    "    \"Gradient Norm\": colors[3],\n",
    "    \"Loss\": colors[4],\n",
    "    \"SVP\": colors[5],\n",
    "    \"Gradient Norm IS\": colors[6],\n",
    "    \"BALD\": colors[2], \n",
    "    \"Entropy\": colors[3], \n",
    "    \"Conditional Entropy\": colors[4],\n",
    "    \"Loss Minus Conditional Entropy\": colors[5]\n",
    "}\n",
    "\n",
    "metrics_color_dict = {\n",
    "    \"Reducible Loss\": colors[0], \n",
    "    \"Gradient Norm\": colors[3],\n",
    "    \"Loss\": colors[4],\n",
    "}\n",
    "\n",
    "def str_to_selection_method_from_dict(string, name_dict):\n",
    "    for k, v in name_dict.items():\n",
    "        if k in string:\n",
    "            return v\n",
    "    return f\"{string} not found\"\n",
    "        \n",
    "def run_to_selection_method(run):\n",
    "    if run.config[\"logger/wandb/project\"] == \"svp_final\":\n",
    "        return \"SVP\"\n",
    "    elif run.config[\"model/_target_\"] == \"src.models.ImportanceSamplingModel.ImportanceSamplingModel\":\n",
    "        return \"Gradient Norm IS\"\n",
    "    else:\n",
    "        name_dict  = {\n",
    "            \"src.curricula.selection_methods.uniform_selection\": \"Uniform Sampling\", \n",
    "            \"src.curricula.selection_methods.reducible_loss_selection\": \"Reducible Loss (Ours)\", \n",
    "            \"src.curricula.selection_methods.irreducible_loss_selection\": \"Irreducible Loss\", \n",
    "            \"src.curricula.selection_methods.gradnorm_ub_selection\": \"Gradient Norm\", \n",
    "            \"src.curricula.selection_methods.ce_loss_selection\": \"Loss\",\n",
    "            \"src.curricula.selection_methods.bald_selection\": \"BALD\",\n",
    "            \"src.curricula.selection_methods.entropy_selection\": \"Entropy\",\n",
    "            \"src.curricula.selection_methods.conditional_entropy_selection\": \"Conditional Entropy\",\n",
    "            \"src.curricula.selection_methods.loss_minus_conditional_entropy_selection\": \"Loss Minus Conditional Entropy\"\n",
    "        }\n",
    "        return str_to_selection_method_from_dict(run.config[\"selection_method/_target_\"], name_dict)\n",
    "        \n",
    "def df_to_xvals(df):\n",
    "    return df[\"trainer/global_step\"].to_numpy()\n",
    "\n",
    "def compute_speedup(runs_all_info, baseline_name, ours_name):\n",
    "    baseline_dfs = [d for _, c, d in runs_all_info if c == baseline_name]\n",
    "    ours_dfs = [d for _, c, d in runs_all_info if c == ours_name]\n",
    "            \n",
    "    x_vals_baseline = df_to_xvals(baseline_dfs[0])\n",
    "    y_vals_baseline = np.zeros(shape=(x_vals_baseline.size, len(baseline_dfs)))\n",
    "    for sm_df_i, sm_df in enumerate(baseline_dfs):\n",
    "        acc_baseline = 100*sm_df[\"val_acc_epoch\"].to_numpy()\n",
    "        y_vals_baseline[:acc_baseline.size, sm_df_i] = acc_baseline\n",
    "    \n",
    "    x_vals_ours = df_to_xvals(ours_dfs[0])\n",
    "    y_vals_ours = np.zeros(shape=(x_vals_ours.size, len(ours_dfs)))\n",
    "    for sm_df_i, sm_df in enumerate(ours_dfs):\n",
    "        acc_ours = 100*sm_df[\"val_acc_epoch\"].to_numpy()\n",
    "        y_vals_ours[:acc_ours.size, sm_df_i] = acc_ours\n",
    "    \n",
    "    baseline_max = np.max(np.mean(y_vals_baseline, axis=-1))\n",
    "    baseline_max_step = x_vals_baseline[np.argmax(np.mean(y_vals_baseline, axis=-1))]\n",
    "    \n",
    "    steps_outperformed = np.zeros(shape=len(ours_dfs))\n",
    "    for run_i in range(len(ours_dfs)):\n",
    "        nzs = np.nonzero(y_vals_ours[:, run_i] > baseline_max)[0]\n",
    "        if len(nzs) > 0:\n",
    "            indx = nzs[0]\n",
    "            steps_outperformed[run_i] = x_vals_ours[indx]\n",
    "        else:\n",
    "            steps_outperformed[run_i] = np.inf\n",
    "            \n",
    "    return(np.mean(baseline_max_step)/np.mean(steps_outperformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea4700a-99b7-4a63-8596-df38a8c9d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_to_dataset(run):\n",
    "    if \"CINIC10\" in run.config[\"datamodule\"]:\n",
    "        return \"CINIC10\"\n",
    "    elif \"CIFAR100\" in run.config[\"datamodule\"]:\n",
    "        return \"CIFAR100\"\n",
    "    elif \"CIFAR10\" in run.config[\"datamodule\"]:\n",
    "        return \"CIFAR10\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd7f605-11e3-4a2f-a8f5-787a5e9bd939",
   "metadata": {},
   "source": [
    "# Active Learning Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6802f18c-ad19-440f-adad-41d08db83c34",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf156a5-9d87-4f58-b5ef-9404ac6e79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_methods = [\"Reducible Loss (Ours)\", \"Uniform Sampling\", \n",
    "                     \"BALD\", \"Entropy\", \"Conditional Entropy\", \"Loss Minus Conditional Entropy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f51b1f-4196-4b91-a361-d436ed76571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d7cdfe-1078-4218-b345-d5854169af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"trainer/global_step\", \"val_acc_epoch\"]\n",
    "\n",
    "mnist_runs = [*filter_runs_by_tag(\"mnist_active_learning\", api.runs(\"goldiprox/mnist_active_learning\"))]\n",
    "mnist_runs_all_info = list(zip(mnist_runs, [run_to_selection_method(r) for r in mnist_runs], [extract_run_df(r, tuple(keys)) for r in mnist_runs]))# convert keys to tuple to allow LRU cache to be used\n",
    "mnist_runs_all_info = [x for x in mnist_runs_all_info if not x[2].empty]\n",
    "\n",
    "cifar_runs = [*filter_runs_by_tag(\"cifar10_active_learning\", api.runs(\"goldiprox/cifar10_active_learning_updatedv2\"))]\n",
    "cifar_runs_all_info = list(zip(cifar_runs, [run_to_selection_method(r) for r in cifar_runs], [extract_run_df(r, tuple(keys)) for r in cifar_runs]))# convert keys to tuple to allow LRU cache to be used\n",
    "cifar_runs_all_info = [x for x in cifar_runs_all_info if not x[2].empty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61df916-1296-46ea-9794-0fdfd57320fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure2a_subplot(selection_methods, runs_all_info, ylim):\n",
    "    for sm_i, sm in enumerate(selection_methods):\n",
    "        sm_dfs = [d for _, c, d in runs_all_info if c == sm]\n",
    "                \n",
    "        if len(sm_dfs) == 0:\n",
    "            print(f\"Could not find any dfs corresponding to {sm}\")\n",
    "            continue\n",
    "            \n",
    "        x_vals = df_to_xvals(sm_dfs[0])\n",
    "        y_vals = np.zeros(shape=(x_vals.size, len(sm_dfs)))\n",
    "        \n",
    "        for sm_df_i, sm_df in enumerate(sm_dfs):\n",
    "            acc = 100*sm_df[\"val_acc_epoch\"].to_numpy()\n",
    "            y_vals[:acc.size, sm_df_i] = acc\n",
    "            \n",
    "        plt.plot(x_vals, np.mean(y_vals, axis=-1), color=selection_methods_color_dict[sm], linewidth=1, label=sm)\n",
    "        plt.fill_between(x_vals, np.min(y_vals, axis=-1), np.max(y_vals, axis=-1), color=selection_methods_color_dict[sm], alpha=0.15, linewidth=0)\n",
    "        plt.xlabel(\"Steps\", fontsize=10)\n",
    "        \n",
    "        plt.ylabel(\"Test Accuracy (%)\", fontsize=10)\n",
    "        plt.xticks(fontsize=8)\n",
    "        plt.yticks(fontsize=8)\n",
    "        plt.ylim(ylim)\n",
    "        plt.xlim([np.min(x_vals), np.max(x_vals)])\n",
    "        \n",
    "def figure2a_subplot_alt(selection_methods, runs_all_info, xlim):\n",
    "    for sm_i, sm in enumerate(selection_methods):\n",
    "        sm_dfs = [d for _, c, d in runs_all_info if c == sm]\n",
    "        if len(sm_dfs) == 0:\n",
    "            print(f\"Could not find any dfs corresponding to {sm}\")\n",
    "            continue\n",
    "            \n",
    "        x_vals = df_to_xvals(sm_dfs[0])\n",
    "        acc_vals = np.zeros(shape=(x_vals.size, len(sm_dfs)))\n",
    "\n",
    "        for sm_df_i, sm_df in enumerate(sm_dfs):\n",
    "            acc_vals[:, sm_df_i] = 100*sm_df[\"val_acc_epoch\"].to_numpy()\n",
    "            \n",
    "        xrange = np.linspace(xlim[0], xlim[1], 50)\n",
    "        steps_needed = np.zeros((xrange.size, len(sm_dfs)))\n",
    "                \n",
    "        for i, acc in enumerate(xrange):\n",
    "            for j in range(len(sm_dfs)):\n",
    "                exceeded_acc = acc_vals[:, j] > acc\n",
    "                if np.sum(exceeded_acc) > 0: # i.e., we exceeded the accuracy\n",
    "                    steps_needed[i, j] = x_vals[np.nonzero(exceeded_acc)[0][0]]\n",
    "                else:\n",
    "                    steps_needed[i, j] = np.nan\n",
    "                    \n",
    "        plt.plot(xrange, np.mean(steps_needed, axis=-1), color=selection_methods_color_dict[sm], linewidth=1.5, label=sm)\n",
    "        plt.fill_between(xrange, np.min(steps_needed, axis=-1), np.max(steps_needed, axis=-1), color=selection_methods_color_dict[sm], alpha=0.15, linewidth=0)\n",
    "        plt.ylabel(\"Steps Required\\nLower is Better\", fontsize=10)\n",
    "        \n",
    "        plt.xlabel(\"Target Accuracy (%)\", fontsize=10)\n",
    "        plt.xticks(fontsize=8)\n",
    "        plt.yticks(fontsize=8)\n",
    "        plt.xlim(xlim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b6c758-192e-4073-aafc-4304cd96ed1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5.75, 2.45), dpi=300)\n",
    "plt.subplot(121)\n",
    "figure2a_subplot(selection_methods, mnist_runs_all_info, [0, 100])\n",
    "plt.title(\"MNIST Active Learning\", fontsize=10)\n",
    "plt.xlim([0, 2000])\n",
    "plt.ylim([75, 98.5])\n",
    "plt.subplot(122)\n",
    "figure2a_subplot(selection_methods, cifar_runs_all_info, [0, 100])\n",
    "plt.title(\"CIFAR Active Learning\", fontsize=10)\n",
    "plt.ylim([15, 75])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.legend(fancybox=True, shadow=True, fontsize=8, loc=\"upper center\", bbox_to_anchor=(-0.15, -0.25), ncol=3)\n",
    "plt.savefig(\"figure_outputs/figure_al_baselines.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce1911f-8275-4bb8-bc8a-133855a4acb9",
   "metadata": {},
   "source": [
    "# Figure 2a—alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad55bf5-2ed3-4f82-9dfe-789d494c7cb3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5.75, 2), dpi=300)\n",
    "plt.subplot(131)\n",
    "figure2a_subplot_alt(selection_methods, cifar10_runs_all_info, [0, 110])\n",
    "plt.title(\"(a) CIFAR10\", fontsize=10)\n",
    "print(f\"CIFAR10 speedup: {compute_speedup(cifar10_runs_all_info, 'Uniform Sampling', 'Reducible Loss (Ours)'):.2f}\")\n",
    "plt.subplot(132)\n",
    "figure2a_subplot_alt(selection_methods, cifar100_runs_all_info, [0, 70])\n",
    "plt.title(\"(b) CIFAR100\", fontsize=10)\n",
    "plt.ylabel(None)\n",
    "print(f\"CIFAR100 speedup: {compute_speedup(cifar100_runs_all_info, 'Uniform Sampling', 'Reducible Loss (Ours)'):.2f}\")\n",
    "plt.subplot(133)\n",
    "figure2a_subplot_alt(selection_methods, cinic10_runs_all_info, [0, 90])\n",
    "plt.title(\"(c) CINIC10\", fontsize=10)\n",
    "plt.ylabel(None)\n",
    "plt.ylim([0, 20000])\n",
    "print(f\"CINIC10 speedup: {compute_speedup(cinic10_runs_all_info, 'Uniform Sampling', 'Reducible Loss (Ours)'):.2f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend(fancybox=True, shadow=True, fontsize=8, ncol=3, bbox_to_anchor=(-1, -0.35), loc=\"upper center\")\n",
    "plt.savefig(\"figure_outputs/figure_2a.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae977a1-8678-400c-b428-8b4d86945036",
   "metadata": {},
   "source": [
    "# Figure 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21c02f3-e274-46cb-bffe-331cae05211d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keys = [\"trainer/global_step\", \"val_acc_epoch\"]\n",
    "\n",
    "cifar10_runs_2b = [*filter_runs_by_tag(\"cifar10_labelnoise\", api.runs(\"goldiprox/jb_cifar10\")), *filter_runs_by_tag(\"cifar10_labelnoise\", api.runs(\"goldiprox/svp_final\"))]\n",
    "cifar10_runs_all_info_2b = list(zip(cifar10_runs_2b, [run_to_selection_method(r) for r in cifar10_runs_2b], [extract_run_df(r, tuple(keys)) for r in cifar10_runs_2b]))# convert keys to tuple to allow LRU cache to be used\n",
    "\n",
    "cinic10_runs_2b = [*filter_runs_by_tag(\"cinic10_labelnoise\", api.runs(\"goldiprox/goldiprox\")), *filter_runs_by_tag(\"cinic10_labelnoise\", api.runs(\"goldiprox/svp_final\"))]\n",
    "cinic10_runs_all_info_2b = list(zip(cinic10_runs_2b, [run_to_selection_method(r) for r in cinic10_runs_2b], [extract_run_df(r, tuple(keys)) for r in cinic10_runs_2b]))# convert keys to tuple to allow LRU cache to be used\n",
    "\n",
    "cifar100_runs_2b = [*filter_runs_by_tag(\"cifar100_labelnoise\", api.runs(\"goldiprox/cifar100\")), *filter_runs_by_tag(\"cifar100_labelnoise\", api.runs(\"goldiprox/svp_final\"))]\n",
    "cifar100_runs_all_info_2b = list(zip(cinic10_runs_2b, [run_to_selection_method(r) for r in cifar100_runs_2a], [extract_run_df(r, tuple(keys)) for r in cifar100_runs_2b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48636cb6-0c2a-4bbf-93ad-beb269992761",
   "metadata": {},
   "source": [
    "# Figure 2b – alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f75f3e-faec-4301-98dc-ee71e28dafd7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5.75, 2), dpi=300)\n",
    "plt.subplot(131)\n",
    "figure2a_subplot_alt(selection_methods, cinic10_runs_all_info_2b, [0, 110])\n",
    "plt.title(\"CIFAR10\", fontsize=10)\n",
    "print(f\"CIFAR10 speedup: {compute_speedup(cifar10_runs_all_info_2b, 'Uniform Sampling', 'Reducible Loss (Ours)'):.2f}\")\n",
    "plt.subplot(132)\n",
    "figure2a_subplot_alt(selection_methods, cifar100_runs_all_info_2b, [0, 70])\n",
    "plt.title(\"CIFAR100\", fontsize=10)\n",
    "plt.ylabel(None)\n",
    "print(f\"CIFAR100 speedup: {compute_speedup(cifar100_runs_all_info_2b, 'Uniform Sampling', 'Reducible Loss (Ours)'):.2f}\")\n",
    "plt.subplot(133)\n",
    "figure2a_subplot_alt(selection_methods, cinic10_runs_all_info_2b, [0, 90])\n",
    "print(f\"CINIC10 speedup: {compute_speedup(cinic10_runs_all_info_2b, 'Uniform Sampling', 'Reducible Loss (Ours)'):.2f}\")\n",
    "plt.title(\"CINIC10\", fontsize=10)\n",
    "plt.ylabel(None)\n",
    "plt.tight_layout()\n",
    "# plt.legend(fancybox=True, shadow=True, fontsize=8, ncol=3, bbox_to_anchor=(-1, -0.35), loc=\"upper center\")\n",
    "plt.savefig(\"figure_outputs/figure_2b.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f40f21-1d5a-4cea-9490-eec6f259cb64",
   "metadata": {},
   "source": [
    "# Figure 2 Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a882dc3-3100-4806-8007-8411de647ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5.75, 3.75), dpi=300)\n",
    "plt.subplot(231)\n",
    "figure2a_subplot_alt(selection_methods, cifar10_runs_all_info, [0, 110])\n",
    "plt.title(\"Half of CIFAR10\", fontsize=10)\n",
    "plt.xlabel(None)\n",
    "print(f\"CIFAR10 speedup: {compute_speedup(cifar10_runs_all_info, 'Uniform Sampling', 'Reducible Loss (Ours)'):.2f}\")\n",
    "plt.subplot(232)\n",
    "figure2a_subplot_alt(selection_methods, cifar100_runs_all_info, [0, 70])\n",
    "plt.title(\"Half of CIFAR100\", fontsize=10)\n",
    "plt.ylabel(None)\n",
    "plt.xlabel(None)\n",
    "print(f\"CIFAR100 speedup: {compute_speedup(cifar100_runs_all_info, 'Uniform Sampling', 'Reducible Loss (Ours)'):.2f}\")\n",
    "splt = plt.subplot(233)\n",
    "figure2a_subplot_alt(selection_methods, cinic10_runs_all_info, [0, 90])\n",
    "plt.title(\"CINIC10\", fontsize=10)\n",
    "plt.ylabel(None)\n",
    "plt.xlabel(None)\n",
    "plt.ylim([0, 20000])\n",
    "print(f\"CINIC10 speedup: {compute_speedup(cinic10_runs_all_info, 'Uniform Sampling', 'Reducible Loss (Ours)'):.2f}\")\n",
    "\n",
    "plt.subplot(234)\n",
    "figure2a_subplot_alt(selection_methods, cifar10_runs_all_info_2b, [0, 100])\n",
    "plt.title(\"Half of CIFAR10\\n(Label Noise)\", fontsize=10)\n",
    "print(f\"CIFAR10 speedup: {compute_speedup(cifar10_runs_all_info_2b, 'Uniform Sampling', 'Reducible Loss (Ours)'):.2f}\")\n",
    "plt.subplot(235)\n",
    "figure2a_subplot_alt(selection_methods, cifar100_runs_all_info_2b, [0, 70])\n",
    "plt.title(\"Half of CIFAR100\\n(Label Noise)\", fontsize=10)\n",
    "plt.ylabel(None)\n",
    "print(f\"CIFAR100 speedup: {compute_speedup(cifar100_runs_all_info_2b, 'Uniform Sampling', 'Reducible Loss (Ours)'):.2f}\")\n",
    "plt.subplot(236)\n",
    "figure2a_subplot_alt(selection_methods, cinic10_runs_all_info_2b, [0, 90])\n",
    "print(f\"CINIC10 speedup: {compute_speedup(cinic10_runs_all_info_2b, 'Uniform Sampling', 'Reducible Loss (Ours)'):.2f}\")\n",
    "plt.title(\"CINIC10\\n(Label Noise)\", fontsize=10)\n",
    "plt.ylabel(None)\n",
    "plt.ylim([0, 20000])\n",
    "plt.tight_layout()\n",
    "\n",
    "splt.legend(fancybox=True, shadow=True, fontsize=8, ncol=3, bbox_to_anchor=(-1, -2.05), loc=\"upper center\")\n",
    "plt.savefig(\"figure_outputs/figure_2_combined.pdf\", bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
